---
title: "Konnect Data Plane Elasticity"
chapter: true
draft: false
weight: 2
---

# Konnect Data Plane Elasticity

One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimaly support a given throughtput. 

This capability is specially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.

Here's our deployment before scaling it out:

```
$ kubectl get service -n kong-dp
NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                      AGE
kong-dp-kong-proxy   LoadBalancer   10.100.56.103   a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com   80:31032/TCP,443:30651/TCP   25m
```

If we check our service, we'll see it has a single endpoint to IP address <b>192.168.16.247</b>
```
$ kubectl describe service kong-dp-kong-proxy -n kong-dp
Name:                     kong-dp-kong-proxy
Namespace:                kong-dp
Labels:                   app.kubernetes.io/instance=kong-dp
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=kong
                          app.kubernetes.io/version=2.5
                          enable-metrics=true
                          helm.sh/chart=kong-2.3.0
Annotations:              meta.helm.sh/release-name: kong-dp
                          meta.helm.sh/release-namespace: kong-dp
Selector:                 app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.56.103
IPs:                      10.100.56.103
LoadBalancer Ingress:     a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com
Port:                     kong-proxy  80/TCP
TargetPort:               8000/TCP
NodePort:                 kong-proxy  31032/TCP
Endpoints:                192.168.32.5:8000
Port:                     kong-proxy-tls  443/TCP
TargetPort:               8443/TCP
NodePort:                 kong-proxy-tls  30651/TCP
Endpoints:                192.168.32.5:8443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  25m   service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   25m   service-controller  Ensured load balancer
```


In fact, this is the Pod's IP adress of our Data Plane. You can check it out.
```
$ kubectl get pod -n kong-dp -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP             NODE                             NOMINATED NODE   READINESS GATES
kong-dp-kong-848f4984dc-zshwv   1/1     Running   0          25m   192.168.32.5   ip-192-168-42-135.ec2.internal   <none>           <none>
```

## Manual Scaling Out

Now, let's scale the deployment out creating 3 replicas of our Pod

```
kubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=3
```

Check the Deployment again:

````
$ kubectl get pod -n kong-dp -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE                             NOMINATED NODE   READINESS GATES
kong-dp-kong-848f4984dc-8gc9r   1/1     Running   0          10s   192.168.38.129   ip-192-168-42-135.ec2.internal   <none>           <none>
kong-dp-kong-848f4984dc-dqltw   1/1     Running   0          10s   192.168.48.245   ip-192-168-42-135.ec2.internal   <none>           <none>
kong-dp-kong-848f4984dc-zshwv   1/1     Running   0          26m   192.168.32.5     ip-192-168-42-135.ec2.internal   <none>           <none>
````

As we can see, the 2 new Pods have been created and are up and running. If we check our Kubernetes Service again, we'll see it has been updated with the new IP addresses. That allows the Service to implement Load Balancing across the Pod replicas.

```
$ kubectl describe service kong-dp-kong-proxy -n kong-dp
Name:                     kong-dp-kong-proxy
Namespace:                kong-dp
Labels:                   app.kubernetes.io/instance=kong-dp
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=kong
                          app.kubernetes.io/version=2.5
                          enable-metrics=true
                          helm.sh/chart=kong-2.3.0
Annotations:              meta.helm.sh/release-name: kong-dp
                          meta.helm.sh/release-namespace: kong-dp
Selector:                 app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.56.103
IPs:                      10.100.56.103
LoadBalancer Ingress:     a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com
Port:                     kong-proxy  80/TCP
TargetPort:               8000/TCP
NodePort:                 kong-proxy  31032/TCP
Endpoints:                192.168.32.5:8000,192.168.38.129:8000,192.168.48.245:8000
Port:                     kong-proxy-tls  443/TCP
TargetPort:               8443/TCP
NodePort:                 kong-proxy-tls  30651/TCP
Endpoints:                192.168.32.5:8443,192.168.38.129:8443,192.168.48.245:8443
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  26m   service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   26m   service-controller  Ensured load balancer
```

Reduce the number of Pods to 1 again running:

```
kubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=1
```


## HPA - Horizontal Autoscaler

HPA (“Horizontal Pod Autoscaler”) is the Kubernetes resource to automatically control the number of replicas of Pods. With HPA, Kubernetes is able to support the requests produced by the consumers, keeping a given Service Level.

Based on CPU utilization or custom metrics, HPA starts and terminates Pods replicas updating all service data to help on the load balancing policies over those replicas.

HPA is described at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. Also, there's a nice walkthrough at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

Kubernetes defines its own units for cpu and memory. You can read more about it at: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/. We use these units to set our Deployments with HPA.

### Install Metrics Server
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

```
$ kubectl get pod -n kube-system
NAME                              READY   STATUS    RESTARTS   AGE
aws-node-kr2c4                    1/1     Running   0          15h
coredns-66cb55d4f4-8pwrn          1/1     Running   0          15h
coredns-66cb55d4f4-hw4mq          1/1     Running   0          15h
kube-proxy-gb8bc                  1/1     Running   0          15h
metrics-server-7b9c4d7fd9-fwlvq   1/1     Running   0          31s
```

### Turn HPA on
Still using Helm, let's upgrade our Data Plane deployment including new and specific settings for HPA:
```
....
--set resources.requests.cpu="300m" \
--set resources.requests.memory="300Mi" \
--set resources.limits.cpu="1200m" \
--set resources.limits.memory="800Mi" \
--set autoscaling.enabled=true \
--set autoscaling.minReplicas=1 \
--set autoscaling.maxReplicas=20 \
--set autoscaling.metrics[0].type=Resource \
--set autoscaling.metrics[0].resource.name=cpu \
--set autoscaling.metrics[0].resource.target.type=Utilization \
--set autoscaling.metrics[0].resource.target.averageUtilization=75
```
The new settings are defining the ammount of CPU and memory each Pod should allocate. At the same time, the "autoscaling" sets are telling HPA how to proceed to instantiate new Pod replicas.


Here's the final Helm command:
```
helm upgrade kong-dp kong/kong -n kong-dp \
--set ingressController.enabled=false \
--set image.repository=kong/kong-gateway \
--set image.tag=2.5.1.0-alpine \
--set env.database=off \
--set env.role=data_plane \
--set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \
--set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \
--set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \
--set proxy.enabled=true \
--set proxy.type=LoadBalancer \
--set enterprise.enabled=true \
--set enterprise.license_secret=kong-enterprise-license \
--set enterprise.portal.enabled=false \
--set enterprise.rbac.enabled=false \
--set enterprise.smtp.enabled=false \
--set manager.enabled=false \
--set portal.enabled=false \
--set portalapi.enabled=false \
--set env.status_listen=0.0.0.0:8100 \
--set secretVolumes[0]=kong-cluster-cert \
--set resources.requests.cpu="300m" \
--set resources.requests.memory="300Mi" \
--set resources.limits.cpu="1200m" \
--set resources.limits.memory="800Mi" \
--set autoscaling.enabled=true \
--set autoscaling.minReplicas=1 \
--set autoscaling.maxReplicas=20 \
--set autoscaling.metrics[0].type=Resource \
--set autoscaling.metrics[0].resource.name=cpu \
--set autoscaling.metrics[0].resource.target.type=Utilization \
--set autoscaling.metrics[0].resource.target.averageUtilization=75
```



### Checking HPA

After submitting the command check the Deployment again. Since we're not consume the Data Plane, we are supposed to see a single Pod running. In the next sections we're going to send requests to the Data Plane and new Pod will get created to handle them.

```
$ kubectl get pod -n kong-dp
NAME                            READY   STATUS        RESTARTS   AGE
kong-dp-kong-67995ddc8c-4vhph   1/1     Running       0          14s
```

You can check the HPA status with:

```
$ kubectl get hpa -n kong-dp
NAME           REFERENCE                 TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
kong-dp-kong   Deployment/kong-dp-kong   0%/75%    1         20        1          46s
```


If you want to change the policy run, for example:
```
kubectl -n kong-dp patch hpa kong-dp-kong --patch '{"spec":{"targetCPUUtilizationPercentage":60}}'
```
or
```
kubectl -n kong-dp patch hpa kong-dp-kong --patch '{"spec":{"maxReplicas":15}}'
```

### Turn HPA off
If you want to turn HPA on we just have to run another Helm upgrade against our Deployment:

```
helm upgrade kong-dp kong/kong -n kong-dp \
--set ingressController.enabled=false \
--set image.repository=kong/kong-gateway \
--set image.tag=2.5.1.0-alpine \
--set env.database=off \
--set env.role=data_plane \
--set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \
--set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \
--set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \
--set proxy.enabled=true \
--set proxy.type=LoadBalancer \
--set enterprise.enabled=true \
--set enterprise.portal.enabled=false \
--set enterprise.rbac.enabled=false \
--set enterprise.smtp.enabled=false \
--set manager.enabled=false \
--set portal.enabled=false \
--set portalapi.enabled=false \
--set env.status_listen=0.0.0.0:8100 \
--set secretVolumes[0]=kong-cluster-cert
```

or deleting the HPA with kubectl
```
kubectl delete hpa kong-dp-kong -n kong-dp
```

Leave the HPA set so we can see it in action when sending requests to the Data Plane.