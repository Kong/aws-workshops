---
title: "Konnect Data Plane Elasticity"
chapter: true
draft: false
weight: 2
---

# Konnect Data Plane Elasticity

One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimaly support a given throughtput. 

This capability is specially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.

Here's our deployment before scaling it out:

```
$ kubectl get service -n kong-dp
NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                                 PORT(S)                      AGE
kong-dp-kong-proxy   LoadBalancer   10.100.169.234   a709304ada39d4c43a45eb22d27e4b8c-161423680.eu-central-1.elb.amazonaws.com   80:32203/TCP,443:30361/TCP   91m
```

If we check our service, we'll see it has a single endpoint to IP address <b>192.168.48.229</b>
```
$ kubectl describe service kong-dp-kong-proxy -n kong-dp
Name:                     kong-dp-kong-proxy
Namespace:                kong-dp
Labels:                   app.kubernetes.io/instance=kong-dp
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=kong
                          app.kubernetes.io/version=2.4
                          enable-metrics=true
                          helm.sh/chart=kong-2.1.0
Annotations:              meta.helm.sh/release-name: kong-dp
                          meta.helm.sh/release-namespace: kong-dp
Selector:                 app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong
Type:                     LoadBalancer
IP Families:              <none>
IP:                       10.100.169.234
IPs:                      10.100.169.234
LoadBalancer Ingress:     a709304ada39d4c43a45eb22d27e4b8c-161423680.eu-central-1.elb.amazonaws.com
Port:                     kong-proxy  80/TCP
TargetPort:               8000/TCP
NodePort:                 kong-proxy  32203/TCP
Endpoints:                192.168.48.229:8000
Port:                     kong-proxy-tls  443/TCP
TargetPort:               8443/TCP
NodePort:                 kong-proxy-tls  30361/TCP
Endpoints:                192.168.48.229:8443
Session Affinity:         None
External Traffic Policy:  Cluster
```


In fact, this is the Pod's IP adress of our Data Plane. You can check it out.
```
$ kubectl get pod -n kong-dp -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE                                             NOMINATED NODE   READINESS GATES
kong-dp-kong-68f445c89d-f2sk9   1/1     Running   0          92m   192.168.48.229   ip-192-168-41-56.eu-central-1.compute.internal   <none>           <none>
```

## Manual Scaling Out

Now, let's scale the deployment out creating 3 replicas of our Pod

```
kubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=3
```

Check the Deployment again:

````
$ kubectl get pod -n kong-dp -o wide
NAME                            READY   STATUS    RESTARTS   AGE    IP               NODE                                             NOMINATED NODE   READINESS GATES
kong-dp-kong-68f445c89d-f2sk9   1/1     Running   0          104m   192.168.48.229   ip-192-168-41-56.eu-central-1.compute.internal   <none>           <none>
kong-dp-kong-68f445c89d-p465k   1/1     Running   0          111s   192.168.39.84    ip-192-168-41-56.eu-central-1.compute.internal   <none>           <none>
kong-dp-kong-68f445c89d-v55zn   1/1     Running   0          111s   192.168.58.165   ip-192-168-41-56.eu-central-1.compute.internal   <none>           <none>
````

As we can see, the 2 new Pods have been created and are up and running. If we check our Kubernetes Service again, we'll see it has been updated with the new IP addresses. That allows the Service to implement Load Balancing across the Pod replicas.

```
$ kubectl describe service kong-dp-kong-proxy -n kong-dp
Name:                     kong-dp-kong-proxy
Namespace:                kong-dp
Labels:                   app.kubernetes.io/instance=kong-dp
                          app.kubernetes.io/managed-by=Helm
                          app.kubernetes.io/name=kong
                          app.kubernetes.io/version=2.4
                          enable-metrics=true
                          helm.sh/chart=kong-2.1.0
Annotations:              meta.helm.sh/release-name: kong-dp
                          meta.helm.sh/release-namespace: kong-dp
Selector:                 app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong
Type:                     LoadBalancer
IP Families:              <none>
IP:                       10.100.169.234
IPs:                      10.100.169.234
LoadBalancer Ingress:     a709304ada39d4c43a45eb22d27e4b8c-161423680.eu-central-1.elb.amazonaws.com
Port:                     kong-proxy  80/TCP
TargetPort:               8000/TCP
NodePort:                 kong-proxy  32203/TCP
Endpoints:                192.168.39.84:8000,192.168.48.229:8000,192.168.58.165:8000
Port:                     kong-proxy-tls  443/TCP
TargetPort:               8443/TCP
NodePort:                 kong-proxy-tls  30361/TCP
Endpoints:                192.168.39.84:8443,192.168.48.229:8443,192.168.58.165:8443
Session Affinity:         None
External Traffic Policy:  Cluster
```

Reduce the number of Pods to 1 again running:

```
kubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=1
```


## HPA - Horizontal Autoscaler

HPA (“Horizontal Pod Autoscaler”) is the Kubernetes resource to automatically control the number of replicas of Pods. With HPA, Kubernetes is able to support the requests produced by the consumers, keeping a given Service Level.

Based on CPU utilization or custom metrics, HPA starts and terminates Pods replicas updating all service data to help on the load balancing policies over those replicas.

HPA is described at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. Also, there's a nice walkthrough at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

Kubernetes defines its own units for cpu and memory. You can read more about it at: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/. We use these units to set our Deployments with HPA.

### Install Metrics Server
```
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

```
$ kubectl get pod -n kube-system
NAME                             READY   STATUS    RESTARTS   AGE
aws-node-ljf7m                   1/1     Running   0          7h27m
coredns-85cc4f6d5-7sbhc          1/1     Running   0          8h
coredns-85cc4f6d5-vkqdv          1/1     Running   0          8h
kube-proxy-tv6rb                 1/1     Running   0          7h27m
metrics-server-9f459d97b-7cnfp   1/1     Running   0          84s
```

### Turn HPA on
Still using Helm, let's upgrade our Data Plane deployment including new and specific settings for HPA:
```
....
--set resources.requests.cpu="300m" \
--set resources.requests.memory="300Mi" \
--set resources.limits.cpu="1200m" \
--set resources.limits.memory="800Mi" \
--set autoscaling.enabled=true \
--set autoscaling.minReplicas=1 \
--set autoscaling.maxReplicas=20 \
--set autoscaling.metrics[0].type=Resource \
--set autoscaling.metrics[0].resource.name=cpu \
--set autoscaling.metrics[0].resource.target.type=Utilization \
--set autoscaling.metrics[0].resource.target.averageUtilization=75
```
The new settings are defining the ammount of CPU and memory each Pod should allocate. At the same time, the "autoscaling" sets are telling HPA how to proceed to instantiate new Pod replicas.


Here's the final Helm command:
```
helm upgrade kong-dp kong/kong -n kong-dp \
--set ingressController.enabled=false \
--set image.repository=kong/kong-gateway \
--set image.tag=2.4.1.1-alpine \
--set env.database=off \
--set env.role=data_plane \
--set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \
--set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \
--set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \
--set proxy.enabled=true \
--set proxy.type=LoadBalancer \
--set enterprise.enabled=true \
--set enterprise.license_secret=kong-enterprise-license \
--set enterprise.portal.enabled=false \
--set enterprise.rbac.enabled=false \
--set enterprise.smtp.enabled=false \
--set manager.enabled=false \
--set portal.enabled=false \
--set portalapi.enabled=false \
--set env.status_listen=0.0.0.0:8100 \
--set secretVolumes[0]=kong-cluster-cert \
--set resources.requests.cpu="300m" \
--set resources.requests.memory="300Mi" \
--set resources.limits.cpu="1200m" \
--set resources.limits.memory="800Mi" \
--set autoscaling.enabled=true \
--set autoscaling.minReplicas=1 \
--set autoscaling.maxReplicas=20 \
--set autoscaling.metrics[0].type=Resource \
--set autoscaling.metrics[0].resource.name=cpu \
--set autoscaling.metrics[0].resource.target.type=Utilization \
--set autoscaling.metrics[0].resource.target.averageUtilization=75
```

### Checking HPA

After submitting the command check the Deployment again. Since we're not consume the Data Plane, we are supposed to see a single Pod running. In the next sections we're going to send requests to the Data Plane and new Pod will get created to handle them.

```
$ kubectl get pod -n kong-dp
NAME                            READY   STATUS        RESTARTS   AGE
kong-dp-kong-58f7b865fb-btktv   1/1     Running       0          24s
```

You can check the HPA status with:

```
$ kubectl get hpa -n kong-dp
NAME           REFERENCE                 TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
kong-dp-kong   Deployment/kong-dp-kong   0%/75%   1         20        1          7m38s
```


If you want to change the policy run, for example:
```
kubectl -n kong-dp patch hpa kong-dp-kong --patch '{"spec":{"targetCPUUtilizationPercentage":60}}'
```
or
```
kubectl -n kong-dp patch hpa kong-dp-kong --patch '{"spec":{"maxReplicas":15}}'
```

### Turn HPA off
If you want to turn HPA on we just have to run another Helm upgrade against our Deployment:

```
helm install kong-dp kong/kong -n kong-dp \
--set ingressController.enabled=false \
--set image.repository=kong/kong-gateway \
--set image.tag=2.4.1.1-alpine \
--set env.database=off \
--set env.role=data_plane \
--set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \
--set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \
--set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \
--set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \
--set proxy.enabled=true \
--set proxy.type=LoadBalancer \
--set enterprise.enabled=true \
--set enterprise.portal.enabled=false \
--set enterprise.rbac.enabled=false \
--set enterprise.smtp.enabled=false \
--set manager.enabled=false \
--set portal.enabled=false \
--set portalapi.enabled=false \
--set env.status_listen=0.0.0.0:8100 \
--set secretVolumes[0]=kong-cluster-cert
```

Leave the HPA set so we can it in action when sending requests to the Data Plane.