[
{
	"uri": "/070_kong_cognito/071_cognito.html",
	"title": "AWS Cognito",
	"tags": [],
	"description": "",
	"content": " Creating AWS Cognito First of all, let\u0026rsquo;s create a Cognito instance using the AWS Console\n Go to Cognito console and click on \u0026ldquo;Managed User Pools\u0026rdquo; and on \u0026ldquo;Create a user pool\u0026rdquo;. Name your pool as \u0026ldquo;kongpool\u0026rdquo; and click on \u0026ldquo;Step through settings\u0026rdquo;. Select “Email address or phone number” and, under that, select “Allow email addresses”. Select the following standard attributes as required  email family name given name  Click on \u0026ldquo;Next step\u0026rdquo;. For the next pages, \u0026ldquo;Policies\u0026rdquo;, “MFA and verifications”, “Message customizations” and \u0026ldquo;Tags\u0026rdquo;, click on \u0026ldquo;Next step\u0026rdquo;. In the page \u0026ldquo;Devices\u0026rdquo;, select “No” for “Do you want to remember your user’s devices” and click on “Next step”. In the pages \u0026ldquo;App clients\u0026rdquo; and \u0026ldquo;Triggers\u0026rdquo; click on \u0026ldquo;Next step\u0026rdquo;. In the page \u0026ldquo;Review\u0026rdquo; click on \u0026ldquo;Create pool\u0026rdquo;. Take note of the \u0026ldquo;Pool Id\u0026rdquo;, in our case \u0026ldquo;us-east-1_XZkYwawRq\u0026rdquo;.  Application Definition  Click on \u0026ldquo;App clients\u0026rdquo; left menu option.\n Click on \u0026ldquo;Add an app client\u0026rdquo; and enter with the following data:\n App client name: kong-api Refresh token expiration (days): 30 Generate client secret: on Enable lambda trigger based custom authentication (ALLOW_CUSTOM_AUTH): off Enable username password based authentication (ALLOW_USER_PASSWORD_AUTH): on Enable SRP (secure remote password) protocol based authentication (ALLOW_USER_SRP_AUTH): off  Click on \u0026ldquo;Set attribute read and write permissions\u0026rdquo; Uncheck everything except the \u0026ldquo;email\u0026rdquo;, \u0026ldquo;family name\u0026rdquo; and \u0026ldquo;given name\u0026rdquo; fields.\n Click on \u0026ldquo;Create app client\u0026rdquo; and on \u0026ldquo;Show details\u0026rdquo;\n Take note of the \u0026ldquo;App client id\u0026rdquo;. In our case, \u0026ldquo;2bstc80hrpbppslrev646e1g6e\u0026rdquo;\n Click on \u0026ldquo;Details\u0026rdquo; and take note of the \u0026ldquo;App client secret\u0026rdquo;. In our case, \u0026ldquo;hqg1pr8s1khm4thi7n4efk6tdblhr2f4cpre51ct4tvlgbglvql\u0026rdquo;\n Click on \u0026ldquo;Save app client changes\u0026rdquo;\n  Register the Ingress endpoint in Cognito Return to your Cognito User Pool to register the Ingress.\n Click on \u0026ldquo;App integration\u0026rdquo; -\u0026gt; \u0026ldquo;App client settings\u0026rdquo;. Click the “Cognito User Pool”  In the \u0026ldquo;Callback URL(s)\u0026rdquo; field type insert your URLs like this. Note that AWS Cognito doesn’t support HTTP callback URLs. This field should include the Ingresses that you want to secure using AWS Cognito. https://a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello\n Click “Authorization code grant”. Click \u0026ldquo;email\u0026rdquo;, \u0026ldquo;openid\u0026rdquo;, \u0026ldquo;aws.cognito.signin.user.admin\u0026rdquo; and \u0026ldquo;profile\u0026rdquo;. Click on “Save changes”. Click on \u0026ldquo;Choose domain name\u0026rdquo;.  In the \u0026ldquo;Domain prefix\u0026rdquo; field type \u0026ldquo;kongidp\u0026rdquo; and click on \u0026ldquo;Check availability\u0026rdquo; to make sure it\u0026rsquo;s available.\n Click on \u0026ldquo;Save changes\u0026rdquo;.  Test the Ingress using HTTP/S  $ http --verify=no https://a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 30 Sep 2021 20:53:56 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.5.1.0-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 2 Hello World, Kong: 2021-09-30 20:53:56.881571  "
},
{
	"uri": "/",
	"title": "AWS Modernization Workshop",
	"tags": [],
	"description": "",
	"content": " AWS Modernization Workshop with Kong Welcome In this workshop you will learn why API Gateway is a pattern for modernization and how to use patterns with Kong to:\n Decentralize Applications and Services - Break down monoliths into services or build new applications with distributed architectures to accelerate your journey to microservices, Kubernetes and Service Mesh. Secure and Govern APIs and Services - Provide consistent security, governance, and compliance across APIs and services. Create a Developer Platform - Provide an internal developer platform built for distributed architectures. Offload critical and complex processing out of services and microservices.  Kong Konnect Enterprise is a Service Connectivity platform that provides technology teams at multi-cloud and hybrid organizations the “architectural freedom” to build APIs and services anywhere. Kong’s Service Connectivity Platform provides a ﬂexible, technology-agnostic platform that supports any cloud, platform, protocol and architecture. Kong Konnect Enterprise supports the full lifecycle of service management, enabling users to easily design, test, secure, deploy, monitor, monetize and version their APIs.\nThe examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.  "
},
{
	"uri": "/010_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Learning Objectives Today we are going to learn the following topics:\n How to deploy Kong Konnect Enterprise in an AWS Elastic Kubernetes Service (EKS) Cluster to expose and protect services.  Workshop Structure This workshop is broken into the sections list below. Estimated time for completing the workshop is 2.5-3.5 hours.\n AWS Setup (30 min) Kong for Kubernetes Setup (40 min) Kong Ingress Creation (30 min) Kong Ingress Policies (40 min) Kong Ingress Consumption and Monitoring (30 min) Cleanup (5 min)  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n "
},
{
	"uri": "/040_kong_enterprise_setup/041_data_plane_and_control_plane_setup.html",
	"title": "Konnect Control Plane and Data Plane Setup",
	"tags": [],
	"description": "",
	"content": " Konnect Control Plane and Data Plane Create the Digital Certificate and Private Key pair In Hybrid mode, a mutual TLS handshake (mTLS) is used for authentication so the actual private key is never transferred on the network, and communication between CP and DP nodes is secure.\nBefore using Hybrid mode, you need a certificate/key pair. Kong Gateway provides two modes for handling certificate/key pairs:\n Shared mode: (Default) Use the Kong CLI to generate a certificate/key pair, then distribute copies across nodes. The certificate/key pair is shared by both CP and DP nodes.\n PKI mode: Provide certificates signed by a central certificate authority (CA). Kong validates both sides by checking if they are from the same CA. This eliminates the risks associated with transporting private keys.\n  To have an easier deployment we\u0026rsquo;re going to use the Shared Mode and OpenSSL to issue the pair. The command below creates two files \u0026ldquo;cluster.key\u0026rdquo; and \u0026ldquo;cluster.crt\u0026rdquo;.\n openssl req -new -x509 -nodes -newkey ec:Configure Kong Konnect Helm Charts to install the Control Plane and Data Plane Before installing the Control Plane make sure you have Helm installed locally:  $ helm version version.BuildInfo{Version:\u0026ldquo;v3.7.0\u0026rdquo;, GitCommit:\u0026ldquo;eeac83883cb4014fe60267ec6373570374ce770b\u0026rdquo;, GitTreeState:\u0026ldquo;clean\u0026rdquo;, GoVersion:\u0026ldquo;go1.17\u0026rdquo;} \nNow add Kong Helm Charts repo:  $ helm repo add kong https://charts.kong.com \u0026ldquo;kong\u0026rdquo; has been added to your repositories \nYou should see it:  $ helm repo ls NAME URL\nkong https://charts.konghq.com \nIf you want to update it run:  $ helm repo update Hang tight while we grab the latest from your chart repositories\u0026hellip; \u0026hellip;Successfully got an update from the \u0026ldquo;kong\u0026rdquo; chart repository Update Complete. ⎈ Happy Helming!⎈ \nControl Plane Let\u0026rsquo;s get started deploying the Kong Konnect Control Plane. First of all, create a \u0026ldquo;kong\u0026rdquo; namespace:  kubectl create namespace kong \nCreate a Kubernetes secret with the pair  kubectl create secret tls kong-cluster-cert \\-\\-cert=./cluster.crt \\-\\-key=./cluster.key -n kong  Create a secret with your license file  kubectl create secret generic kong-enterprise-license -n kong --from-file=./license  Create a \u0026ldquo;admin_gui.session_conf\u0026rdquo; file for Kong Manager session conf.  {\"cookie_name\":\"admin_session\",\"cookie_samesite\":\"off\",\"secret\":\"kong\",\"cookie_secure\":false,\"storage\":\"kong\"}  Create a \u0026ldquo;portal_session_conf\u0026rdquo; file for for Kong DevPortal session conf  {\"cookie_name\":\"portal_session\",\"cookie_samesite\":\"off\",\"secret\":\"kong\",\"cookie_secure\":false,\"storage\":\"kong\"}  Create the session conf with kubectl  kubectl create secret generic kong-session-config -n kong \u0026ndash;from-file=admin_gui_session_conf \u0026ndash;from-file=portal_session_conf \nDeploy the Control Plane helm install kong kong/kong -n kong \u0026ndash;set env.database=postgres \u0026ndash;set env.role=control_plane \u0026ndash;set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \u0026ndash;set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \u0026ndash;set cluster.enabled=true \u0026ndash;set cluster.tls.enabled=true \u0026ndash;set cluster.tls.servicePort=8005 \u0026ndash;set cluster.tls.containerPort=8005 \u0026ndash;set clustertelemetry.enabled=true \u0026ndash;set clustertelemetry.tls.enabled=true \u0026ndash;set clustertelemetry.tls.servicePort=8006 \u0026ndash;set clustertelemetry.tls.containerPort=8006 \u0026ndash;set image.repository=kong/kong-gateway \u0026ndash;set image.tag=2.5.1.0-alpine \u0026ndash;set admin.enabled=true \u0026ndash;set admin.http.enabled=true \u0026ndash;set admin.type=LoadBalancer \u0026ndash;set proxy.enabled=true \u0026ndash;set proxy.type=ClusterIP \u0026ndash;set ingressController.enabled=true \u0026ndash;set ingressController.installCRDs=false \u0026ndash;set ingressController.image.repository=kong/kubernetes-ingress-controller \u0026ndash;set ingressController.image.tag=1.3.2-alpine \u0026ndash;set postgresql.enabled=true \u0026ndash;set postgresql.postgresqlUsername=kong \u0026ndash;set postgresql.postgresqlDatabase=kong \u0026ndash;set postgresql.postgresqlPassword=kong \u0026ndash;set enterprise.enabled=true \u0026ndash;set enterprise.license_secret=kong-enterprise-license \u0026ndash;set enterprise.rbac.enabled=false \u0026ndash;set enterprise.smtp.enabled=false \u0026ndash;set enterprise.portal.enabled=true \u0026ndash;set manager.enabled=true \u0026ndash;set manager.type=LoadBalancer \u0026ndash;set portal.enabled=true \u0026ndash;set portal.http.enabled=true \u0026ndash;set env.portal_gui_protocol=http \u0026ndash;set portal.type=LoadBalancer \u0026ndash;set portalapi.enabled=true \u0026ndash;set portalapi.http.enabled=true \u0026ndash;set portalapi.type=LoadBalancer \u0026ndash;set secretVolumes[0]=kong-cluster-cert\nControl Plane uses port 8005 to publish any new API configuration it has. On the other hand, Data Plane uses the port 8006 to report back all metrics regarding API Consumption.\nData Plane Create another Kubernetes namespace specifically for the Data Plane:  kubectl create namespace kong-dp \nCreate the secret for the Data Plane using the same Digital Certicate and Private Key pair:  kubectl create secret tls kong-cluster-cert --cert=./cluster.crt --key=./cluster.key -n kong-dp \nCreate a secret with your license file  kubectl create secret generic kong-enterprise-license -n kong-dp \u0026ndash;from-file=./license \nInstall the Data Plane\n helm install kong-dp kong/kong -n kong-dp \\ --set ingressController.enabled=false \\ --set image.repository=kong/kong-gateway \\ --set image.tag=2.5.1.0-alpine \\ --set env.database=off \\ --set env.role=data_plane \\ --set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \\ --set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \\ --set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \\ --set proxy.enabled=true \\ --set proxy.type=LoadBalancer \\ --set enterprise.enabled=true \\ --set enterprise.license_secret=kong-enterprise-license \\ --set enterprise.portal.enabled=false \\ --set enterprise.rbac.enabled=false \\ --set enterprise.smtp.enabled=false \\ --set manager.enabled=false \\ --set portal.enabled=false \\ --set portalapi.enabled=false \\ --set env.status_listen=0.0.0.0:8100 \\ --set secretVolumes[0]=kong-cluster-cert  Note we\u0026rsquo;re using the Control Plane\u0026rsquo;s Kubernetes FQDN to get the Data Plane connected to it.\nChecking the Installation  $ kubectl get deployment --all-namespaces NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kong-dp kong-dp-kong 1/1 1 1 2m15s kong kong-kong 1/1 1 1 20m kube-system coredns 2/2 2 2 15h   $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kong-dp kong-dp-kong-848f4984dc-zshwv 1/1 Running 0 11m kong kong-kong-777b5f56bc-76k5v 2/2 Running 0 19m kong kong-kong-init-migrations-fwmg7 0/1 Completed 0 29m kong kong-postgresql-0 1/1 Running 0 29m kube-system aws-node-kr2c4 1/1 Running 0 15h kube-system coredns-66cb55d4f4-8pwrn 1/1 Running 0 15h kube-system coredns-66cb55d4f4-hw4mq 1/1 Running 0 15h kube-system kube-proxy-gb8bc 1/1 Running 0 15h   $ kubectl get service --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.100.0.1  443/TCP 15h kong-dp kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 11m kong kong-kong-admin LoadBalancer 10.100.124.36 a9ed78cc5bb954931aec1b5bf48298f6-2098365612.us-east-1.elb.amazonaws.com 8001:32155/TCP,8444:32134/TCP 29m kong kong-kong-cluster ClusterIP 10.100.105.110  8005/TCP 29m kong kong-kong-clustertelemetry ClusterIP 10.100.192.124  8006/TCP 29m kong kong-kong-manager LoadBalancer 10.100.212.94 abd76df53f5584e8f800a9f9ac73d5fa-21140374.us-east-1.elb.amazonaws.com 8002:30693/TCP,8445:30219/TCP 29m kong kong-kong-portal LoadBalancer 10.100.135.213 a4d6e108295f5458e9cffa00856c1fb2-1667372698.us-east-1.elb.amazonaws.com 8003:30474/TCP,8446:32207/TCP 29m kong kong-kong-portalapi LoadBalancer 10.100.153.195 a0e9f107ba17749e5ac1542792a049f2-1349476423.us-east-1.elb.amazonaws.com 8004:31298/TCP,8447:30469/TCP 29m kong kong-kong-proxy ClusterIP 10.100.205.134  80/TCP,443/TCP 29m kong kong-postgresql ClusterIP 10.100.10.254  5432/TCP 29m kong kong-postgresql-headless ClusterIP None  5432/TCP 29m kube-system kube-dns ClusterIP 10.100.0.10  53/UDP,53/TCP 15h  Checking the Kong Konnect Rest Admin API port Use the Load Balancer created during the deployment  $ kubectl get service kong-kong-admin --output=jsonpath=\u0026lsquo;{.status.loadBalancer.ingress[0].hostname}\u0026rsquo; -n kong a9ed78cc5bb954931aec1b5bf48298f6-2098365612.us-east-1.elb.amazonaws.com \n $ http a9ed78cc5bb954931aec1b5bf48298f6-2098365612.us-east-1.elb.amazonaws.com:8001 | jq -r .version 2.5.1.0-enterprise-edition  Checking the Data Plane from the Control Plane  $ http a9ed78cc5bb954931aec1b5bf48298f6-2098365612.us-east-1.elb.amazonaws.com:8001/clustering/status HTTP/1.1 200 OK Access-Control-Allow-Credentials: true Access-Control-Allow-Origin: http://abd76df53f5584e8f800a9f9ac73d5fa-21140374.us-east-1.elb.amazonaws.com:8002 Connection: keep-alive Content-Length: 177 Content-Type: application/json; charset=utf-8 Date: Thu, 30 Sep 2021 14:59:49 GMT Deprecation: true Server: kong/2.5.1.0-enterprise-edition X-Kong-Admin-Latency: 3 X-Kong-Admin-Request-ID: ZG1HJrWBSgCDc2wz42DkGRwzZEDXF0Ia vary: Origin { \"595ec021-5f64-4a10-ade2-0abdb9ffe444\": { \"config_hash\": \"b2c946b21b1a3c5bd3bc72ccdfc5cc78\", \"hostname\": \"kong-dp-kong-848f4984dc-zshwv\", \"ip\": \"192.168.32.5\", \"last_seen\": 1633013975 } }  Checking the Data Plane Proxy Use the Load Balancer created during the deployment\n $ kubectl get svc -n kong-dp kong-dp-kong-proxy --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}' a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com   $ http a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com HTTP/1.1 404 Not Found Connection: keep-alive Content-Length: 48 Content-Type: application/json; charset=utf-8 Date: Thu, 30 Sep 2021 15:00:35 GMT Server: kong/2.5.1.0-enterprise-edition X-Kong-Response-Latency: 0 { \"message\": \"no Route matched with those values\" }  Configuring Kong Manager Service Kong Manager is the Control Plane Admin GUI. It should get the Admin URI configured with the same Load Balancer address:  kubectl patch deployment -n kong kong-kong -p \u0026ldquo;{\\\u0026ldquo;spec\\\u0026rdquo;: { \\\u0026ldquo;template\\\u0026rdquo; : { \\\u0026ldquo;spec\\\u0026rdquo; : {\\\u0026ldquo;containers\\\u0026rdquo;:[{\\\u0026ldquo;name\\\u0026rdquo;:\\\u0026ldquo;proxy\\\u0026ldquo;,\\\u0026ldquo;env\\\u0026rdquo;: [{ \\\u0026ldquo;name\\\u0026rdquo; : \\\u0026ldquo;KONG_ADMIN_API_URI\\\u0026ldquo;, \\\u0026ldquo;value\\\u0026rdquo;: \\\u0026ldquo;a9ed78cc5bb954931aec1b5bf48298f6-2098365612.us-east-1.elb.amazonaws.com:8001\\\u0026rdquo; }]}]}}}}\u0026rdquo; \n kubectl patch deployment -n kong kong-kong -p \"{\\\"spec\\\": { \\\"template\\\" : { \\\"spec\\\" : {\\\"containers\\\":[{\\\"name\\\":\\\"proxy\\\",\\\"env\\\": [{ \\\"name\\\" : \\\"KONG_ADMIN_GUI_URL\\\", \\\"value\\\": \\\"http:\\/\\/abd76df53f5584e8f800a9f9ac73d5fa-21140374.us-east-1.elb.amazonaws.com:8002\\\" }]}]}}}}\"  Configuring Kong Dev Portal  $ kubectl get service kong-kong-portalapi -n kong --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}' a0e9f107ba17749e5ac1542792a049f2-1349476423.us-east-1.elb.amazonaws.com   kubectl patch deployment -n kong kong-kong -p \"{\\\"spec\\\": { \\\"template\\\" : { \\\"spec\\\" : {\\\"containers\\\":[{\\\"name\\\":\\\"proxy\\\",\\\"env\\\": [{ \\\"name\\\" : \\\"KONG_PORTAL_API_URL\\\", \\\"value\\\": \\\"http://a0e9f107ba17749e5ac1542792a049f2-1349476423.us-east-1.elb.amazonaws.com:8004\\\" }]}]}}}}\"   $ kubectl get service kong-kong-portal -n kong --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}' a4d6e108295f5458e9cffa00856c1fb2-1667372698.us-east-1.elb.amazonaws.com   kubectl patch deployment -n kong kong-kong -p \"{\\\"spec\\\": { \\\"template\\\" : { \\\"spec\\\" : {\\\"containers\\\":[{\\\"name\\\":\\\"proxy\\\",\\\"env\\\": [{ \\\"name\\\" : \\\"KONG_PORTAL_GUI_HOST\\\", \\\"value\\\": \\\"a4d6e108295f5458e9cffa00856c1fb2-1667372698.us-east-1.elb.amazonaws.com:8003\\\" }]}]}}}}\"  Logging to Kong Manager Login to Kong Manager using the specific ELB:\n $ kubectl get service kong-kong-manager -n kong --output=jsonpath='{.status.loadBalancer.ingress[0].hostname}' abd76df53f5584e8f800a9f9ac73d5fa-21140374.us-east-1.elb.amazonaws.com  If you redirect your browser to http://abd76df53f5584e8f800a9f9ac73d5fa-21140374.us-east-1.elb.amazonaws.com:8002 you should see the Kong Manager landing page:\nGo to Kong Developer Portal Login to Kong Manager using the specific ELB:\n"
},
{
	"uri": "/060_kong_ingress/061_cache.html",
	"title": "Proxy Caching",
	"tags": [],
	"description": "",
	"content": " Proxy Caching Policy Definition Since we have the Microservice exposed through a route defined in the Ingress Controller, let\u0026rsquo;s apply the Proxy Caching plugin to cache the data coming from it.\nCreate the plugin\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: proxycache namespace: default config: cache_ttl: 30 strategy: memory content_type: [\u0026#34;text/html; charset=utf-8\u0026#34;] plugin: proxy-cache EOF If you want to delete it run:\n$ kubectl delete kongplugin proxycache Apply the plugin to the route\nkubectl patch ingress sampleroute -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;konghq.com/plugins\u0026#34;:\u0026#34;proxycache\u0026#34;}}}\u0026#39; In case you want to disapply the plugin to the ingress run:\n$ kubectl annotate ingress sampleroute konghq.com/plugins- Test the plugin. Since our cache is empty the X-Cache-Status header reports a Miss value. On the other hand, the value was stored in our Cache for further requests.\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 19:56:08 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Cache-Key: f2d45950abe49485a51167bb1d1deae0 X-Cache-Status: Miss X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 Hello World, Kong: 2021-07-08 19:56:08.550405 If we send another request the Header will show Hit meaning the Gateway didn\u0026rsquo;t have to go to the Upstream to satify the request.\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Age: 2 Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 19:56:08 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Cache-Key: f2d45950abe49485a51167bb1d1deae0 X-Cache-Status: Hit X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 0 Hello World, Kong: 2021-07-08 19:56:08.550405 If we wait for the 30 second timeout we configure for the Cache TTL, the Gateway will purge the data from it and respond with a Miss again.\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 19:56:45 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Cache-Key: f2d45950abe49485a51167bb1d1deae0 X-Cache-Status: Miss X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 2 Hello World, Kong: 2021-07-08 19:56:45.227579"
},
{
	"uri": "/020_event_engine_setup.html",
	"title": "Event Engine Setup",
	"tags": [],
	"description": "",
	"content": " Event Engine Welcome to the Event Engine Setup section! This means that you are attending an AWS Hosted Workshop!! If you are not attending a workshop, please go to the self guided setup section. Event Engine is a tool created at AWS that provisions AWS accounts for workshop events like this! These accounts will automatically terminate 24 hours after the workshop begins participants don\u0026rsquo;t have to worry about leaving anything on. Each workshop participant will receive their own Event Engine AWS account.\nYou should have an EKS Cluster running after using the Event Engine tool.\nThe next page will show you how to gain access to your Event Engine dashboard!\n"
},
{
	"uri": "/040_kong_enterprise_setup/042_hpa.html",
	"title": "Konnect Data Plane Elasticity",
	"tags": [],
	"description": "",
	"content": " Konnect Data Plane Elasticity One of the most important capabilities provided by Kubernetes is to easily scale out a Deployment. With a single command we can create or terminate pod replicas in order to optimaly support a given throughtput.\nThis capability is specially interesting for Kubernetes applications like Kong for Kubernetes Ingress Controller.\nHere\u0026rsquo;s our deployment before scaling it out:\n$ kubectl get service -n kong-dp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 25m If we check our service, we\u0026rsquo;ll see it has a single endpoint to IP address 192.168.16.247\n$ kubectl describe service kong-dp-kong-proxy -n kong-dp Name: kong-dp-kong-proxy Namespace: kong-dp Labels: app.kubernetes.io/instance=kong-dp app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=kong app.kubernetes.io/version=2.5 enable-metrics=true helm.sh/chart=kong-2.3.0 Annotations: meta.helm.sh/release-name: kong-dp meta.helm.sh/release-namespace: kong-dp Selector: app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.100.56.103 IPs: 10.100.56.103 LoadBalancer Ingress: a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com Port: kong-proxy 80/TCP TargetPort: 8000/TCP NodePort: kong-proxy 31032/TCP Endpoints: 192.168.32.5:8000 Port: kong-proxy-tls 443/TCP TargetPort: 8443/TCP NodePort: kong-proxy-tls 30651/TCP Endpoints: 192.168.32.5:8443 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 25m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 25m service-controller Ensured load balancer In fact, this is the Pod\u0026rsquo;s IP adress of our Data Plane. You can check it out.\n$ kubectl get pod -n kong-dp -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kong-dp-kong-848f4984dc-zshwv 1/1 Running 0 25m 192.168.32.5 ip-192-168-42-135.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Manual Scaling Out Now, let\u0026rsquo;s scale the deployment out creating 3 replicas of our Pod\nkubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=3 Check the Deployment again:\n$ kubectl get pod -n kong-dp -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kong-dp-kong-848f4984dc-8gc9r 1/1 Running 0 10s 192.168.38.129 ip-192-168-42-135.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kong-dp-kong-848f4984dc-dqltw 1/1 Running 0 10s 192.168.48.245 ip-192-168-42-135.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kong-dp-kong-848f4984dc-zshwv 1/1 Running 0 26m 192.168.32.5 ip-192-168-42-135.ec2.internal \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As we can see, the 2 new Pods have been created and are up and running. If we check our Kubernetes Service again, we\u0026rsquo;ll see it has been updated with the new IP addresses. That allows the Service to implement Load Balancing across the Pod replicas.\n$ kubectl describe service kong-dp-kong-proxy -n kong-dp Name: kong-dp-kong-proxy Namespace: kong-dp Labels: app.kubernetes.io/instance=kong-dp app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=kong app.kubernetes.io/version=2.5 enable-metrics=true helm.sh/chart=kong-2.3.0 Annotations: meta.helm.sh/release-name: kong-dp meta.helm.sh/release-namespace: kong-dp Selector: app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.100.56.103 IPs: 10.100.56.103 LoadBalancer Ingress: a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com Port: kong-proxy 80/TCP TargetPort: 8000/TCP NodePort: kong-proxy 31032/TCP Endpoints: 192.168.32.5:8000,192.168.38.129:8000,192.168.48.245:8000 Port: kong-proxy-tls 443/TCP TargetPort: 8443/TCP NodePort: kong-proxy-tls 30651/TCP Endpoints: 192.168.32.5:8443,192.168.38.129:8443,192.168.48.245:8443 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 26m service-controller Ensuring load balancer Normal EnsuredLoadBalancer 26m service-controller Ensured load balancer Reduce the number of Pods to 1 again running:\nkubectl scale deployment.v1.apps/kong-dp-kong -n kong-dp --replicas=1 HPA - Horizontal Autoscaler HPA (“Horizontal Pod Autoscaler”) is the Kubernetes resource to automatically control the number of replicas of Pods. With HPA, Kubernetes is able to support the requests produced by the consumers, keeping a given Service Level.\nBased on CPU utilization or custom metrics, HPA starts and terminates Pods replicas updating all service data to help on the load balancing policies over those replicas.\nHPA is described at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/. Also, there\u0026rsquo;s a nice walkthrough at https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/\nKubernetes defines its own units for cpu and memory. You can read more about it at: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/. We use these units to set our Deployments with HPA.\nInstall Metrics Server kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml$ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE aws-node-kr2c4 1/1 Running 0 15h coredns-66cb55d4f4-8pwrn 1/1 Running 0 15h coredns-66cb55d4f4-hw4mq 1/1 Running 0 15h kube-proxy-gb8bc 1/1 Running 0 15h metrics-server-7b9c4d7fd9-fwlvq 1/1 Running 0 31s Turn HPA on Still using Helm, let\u0026rsquo;s upgrade our Data Plane deployment including new and specific settings for HPA:\n.... --set resources.requests.cpu=\u0026#34;300m\u0026#34; \\ --set resources.requests.memory=\u0026#34;300Mi\u0026#34; \\ --set resources.limits.cpu=\u0026#34;1200m\u0026#34; \\ --set resources.limits.memory=\u0026#34;800Mi\u0026#34; \\ --set autoscaling.enabled=true \\ --set autoscaling.minReplicas=1 \\ --set autoscaling.maxReplicas=20 \\ --set autoscaling.metrics[0].type=Resource \\ --set autoscaling.metrics[0].resource.name=cpu \\ --set autoscaling.metrics[0].resource.target.type=Utilization \\ --set autoscaling.metrics[0].resource.target.averageUtilization=75 The new settings are defining the ammount of CPU and memory each Pod should allocate. At the same time, the \u0026ldquo;autoscaling\u0026rdquo; sets are telling HPA how to proceed to instantiate new Pod replicas.\nHere\u0026rsquo;s the final Helm command:\nhelm upgrade kong-dp kong/kong -n kong-dp \\ --set ingressController.enabled=false \\ --set image.repository=kong/kong-gateway \\ --set image.tag=2.5.1.0-alpine \\ --set env.database=off \\ --set env.role=data_plane \\ --set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \\ --set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \\ --set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \\ --set proxy.enabled=true \\ --set proxy.type=LoadBalancer \\ --set enterprise.enabled=true \\ --set enterprise.license_secret=kong-enterprise-license \\ --set enterprise.portal.enabled=false \\ --set enterprise.rbac.enabled=false \\ --set enterprise.smtp.enabled=false \\ --set manager.enabled=false \\ --set portal.enabled=false \\ --set portalapi.enabled=false \\ --set env.status_listen=0.0.0.0:8100 \\ --set secretVolumes[0]=kong-cluster-cert \\ --set resources.requests.cpu=\u0026#34;300m\u0026#34; \\ --set resources.requests.memory=\u0026#34;300Mi\u0026#34; \\ --set resources.limits.cpu=\u0026#34;1200m\u0026#34; \\ --set resources.limits.memory=\u0026#34;800Mi\u0026#34; \\ --set autoscaling.enabled=true \\ --set autoscaling.minReplicas=1 \\ --set autoscaling.maxReplicas=20 \\ --set autoscaling.metrics[0].type=Resource \\ --set autoscaling.metrics[0].resource.name=cpu \\ --set autoscaling.metrics[0].resource.target.type=Utilization \\ --set autoscaling.metrics[0].resource.target.averageUtilization=75 Checking HPA After submitting the command check the Deployment again. Since we\u0026rsquo;re not consume the Data Plane, we are supposed to see a single Pod running. In the next sections we\u0026rsquo;re going to send requests to the Data Plane and new Pod will get created to handle them.\n$ kubectl get pod -n kong-dp NAME READY STATUS RESTARTS AGE kong-dp-kong-67995ddc8c-4vhph 1/1 Running 0 14s You can check the HPA status with:\n$ kubectl get hpa -n kong-dp NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE kong-dp-kong Deployment/kong-dp-kong 0%/75% 1 20 1 46s If you want to change the policy run, for example:\nkubectl -n kong-dp patch hpa kong-dp-kong --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;targetCPUUtilizationPercentage\u0026#34;:60}}\u0026#39; or\nkubectl -n kong-dp patch hpa kong-dp-kong --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;maxReplicas\u0026#34;:15}}\u0026#39; Turn HPA off If you want to turn HPA on we just have to run another Helm upgrade against our Deployment:\nhelm upgrade kong-dp kong/kong -n kong-dp \\ --set ingressController.enabled=false \\ --set image.repository=kong/kong-gateway \\ --set image.tag=2.5.1.0-alpine \\ --set env.database=off \\ --set env.role=data_plane \\ --set env.cluster_cert=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_cert_key=/etc/secrets/kong-cluster-cert/tls.key \\ --set env.lua_ssl_trusted_certificate=/etc/secrets/kong-cluster-cert/tls.crt \\ --set env.cluster_control_plane=kong-kong-cluster.kong.svc.cluster.local:8005 \\ --set env.cluster_telemetry_endpoint=kong-kong-clustertelemetry.kong.svc.cluster.local:8006 \\ --set proxy.enabled=true \\ --set proxy.type=LoadBalancer \\ --set enterprise.enabled=true \\ --set enterprise.portal.enabled=false \\ --set enterprise.rbac.enabled=false \\ --set enterprise.smtp.enabled=false \\ --set manager.enabled=false \\ --set portal.enabled=false \\ --set portalapi.enabled=false \\ --set env.status_listen=0.0.0.0:8100 \\ --set secretVolumes[0]=kong-cluster-cert or deleting the HPA with kubectl\nkubectl delete hpa kong-dp-kong -n kong-dp Leave the HPA set so we can see it in action when sending requests to the Data Plane.\n"
},
{
	"uri": "/030_aws_setup_for_hosting_kong.html",
	"title": "AWS Setup for Hosting Kong",
	"tags": [],
	"description": "",
	"content": " Self-Paced Workshop Welcome to the Self Guided Setup section! This workshop requires an AWS account where there is IAM user/identity that has proper permissions to set up the necessary AWS components to work through the workshop. Worried about costs associated with this workshop? Don\u0026rsquo;t worry, go to the next page and request some AWS credits to pay for any costs that may be incurred through this workshop!\nHere is a preview of what we will be setting up:\n Creating an AWS account with proper permissions Requesting AWS credit from AWS Marketplace  Kong Konnect Enterprise Hybrid Mode One of the most powerful capabilities provided by Kong Konnect Enterprise is the support for Hybrid deployments. In other words, it implements distributed API Gateway Clusters with multiple instances running on several environments at the same time.\nMoreover, Kong Konnect Enterprise provides a new topology option, named Hybrid Mode, with a total separation of the Control Plane (CP) and Data Plane (DP). That is, while the Control Plane is responsible for administration tasks, the Data Plane is exclusively used by API Consumers.\nPlease, refer to the following link to read more about the Hybrid deployment: https://docs.konghq.com/enterprise/2.5.x/deployment/hybrid-mode/\nReference Architecture Here\u0026rsquo;s a Reference Architecture implemented in AWS:\n Both Control Plane and Data Plane run on an Elastic Kubernetes Service (EKS) Cluster in different namespaces. PostgreSQL Database is located behind the CP.  Considering the capabilities provided by the Kubernetes platform, running Data Planes on this platform delivers a powerful environment. Here are some capabilities leveraged by the Data Plane on Kubernetes:\n High Availability: One of the main Kubernetes\u0026rsquo; capabilities is \u0026ldquo;Self-Healing\u0026rdquo;. If a \u0026ldquo;pod\u0026rdquo; crashes, Kubernetes takes care of it, reinitializing the \u0026ldquo;pod\u0026rdquo;.\n Scalability/Elasticity: HPA (\u0026ldquo;Horizontal Pod Autoscaler\u0026rdquo;) is the capability to initialize and terminate \u0026ldquo;pod\u0026rdquo; replicas based on previously defined policies. The policies define \u0026ldquo;thresholds\u0026rdquo; to tell Kubernetes the conditions where it should initiate a brand new \u0026ldquo;pod\u0026rdquo; replica or terminate a running one.\n Load Balancing: The Kubernetes Service notion defines an abstraction level on top of the \u0026ldquo;pod\u0026rdquo; replicas that might have been up or down (due HPA policies, for instance). Kubernetes keeps all the \u0026ldquo;pod\u0026rdquo; replicas hidden from the \u0026ldquo;callers\u0026rdquo; through Services.\n  Important remark #1: this tutorial is intended to be used for labs and PoC only. There are many aspects and processes, typically implemented in production sites, not described here. For example: Digital Certificate issuing, Cluster monitoring, etc.\nImportant remark #2: the deployment is based on Kong Enterprise 2.4 running on \u0026ldquo;Free Mode\u0026rdquo;. Please contact Kong to get a Kong Enterprise trial license to use its Enterprise features.\n"
},
{
	"uri": "/070_kong_cognito/073_oidc_plugin.html",
	"title": "OpenId Connect Plugin",
	"tags": [],
	"description": "",
	"content": " Instantiating an OIDC plugin cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: oidc namespace: default config: client_id: [2bstc80hrpbppslrev646e1g6e] client_secret: [hqg1pr8s1khm4thi7n4efk6tdblhr2f4cpre51ct4tvlgbglvql] issuer: \u0026#34;https://cognito-idp.us-east-1.amazonaws.com/us-east-1_XZkYwawRq/.well-known/openid-configuration\u0026#34; cache_ttl: 10 redirect_uri: [\u0026#34;https://a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello\u0026#34;] plugin: openid-connect EOF Observations:\n The issuer URL follows the format: https://cognito-idp.{region}.amazonaws.com/{userPoolId} OIDC plugin generates, by default the \u0026ldquo;redirect uri\u0026rdquo; based on its port (8443). The \u0026ldquo;redirect_uri\u0026rdquo; parameter defines the URI to be used to redirect the user after getting authenticated.  Apply OIDC plugin to the Ingress  kubectl patch ingress sampleroute -p '{\"metadata\":{\"annotations\":{\"konghq.com/plugins\":\"oidc\"}}}' ingress.extensions/httpbin patched  if you want to disable the plugin run:  kubectl annotate ingress sampleroute konghq.com/plugins- \nConsume the Route with a Browser https://a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello\nAfter accepting the Server Certificate, sInce you haven\u0026rsquo;t been authenticated, you will be redirected to Cognito\u0026rsquo;s Authentication page:\nClick on \u0026ldquo;Sign up\u0026rdquo; to register.\nAfter entering your data click on \u0026ldquo;Sign Up\u0026rdquo;. Cognito will create a user and request the verification code sent by your email.\nAfter typing the code, Cognito will authenticate you, issues an Authorization Code and redirects you back to the original URL (Data Plane). The Data Plane connects to Cognito with the Authorization Code to get the Access Token and then allows you to consume the URL.\n"
},
{
	"uri": "/040_kong_enterprise_setup/043_prometheus_grafana.html",
	"title": "Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Prometheus and Grafana From the Observability perspective, we\u0026rsquo;re going to use Prometheus and Grafana. Two levels of monitoring are possible:\n Kubernetes monitoring: Prometheus and Grafana monitor Kong Data Plane Deployment in terms of CPU, memory and networking consumption as well as HPA and the number of Pod replicas, just like any Kubernetes Deployment. Kong Data Plane monitoring: Prometheus and Grafana expose metrics the Kong Data Planes replicas provide in terms of API consumption including number of processed requests, etc.  We\u0026rsquo;re going to use Prometheus Operator to address these two monitoring levels. Moreover, to support HPA from the Observability perspective, we\u0026rsquo;re going to configure Service Monitor to monitor the variable number of Pod replicas.\nPrometheus Operator First of all, let\u0026rsquo;s install Prometheus Operator with its specific Helm Charts. Note we\u0026rsquo;re requesting Load Balancers to expose Prometheus, Grafana and Alert Manager UIs.\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo updatekubectl create namespace prometheushelm install prometheus -n prometheus prometheus-community/kube-prometheus-stack \\ --set alertmanager.service.type=LoadBalancer \\ --set prometheus.service.type=LoadBalancer \\ --set grafana.service.type=LoadBalancer Use should see several new Pods and Services after the installations\n$ kubectl get service -n prometheus NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-operated ClusterIP None \u0026lt;none\u0026gt; 9093/TCP,9094/TCP,9094/UDP 55s prometheus-grafana LoadBalancer 10.100.47.13 a5f6d918221a14383813bc2f6b88c6c4-2050279250.us-east-1.elb.amazonaws.com 80:31877/TCP 59s prometheus-kube-prometheus-alertmanager LoadBalancer 10.100.18.20 acb89669ac9a1432b81c25d418832f84-579800983.us-east-1.elb.amazonaws.com 9093:32325/TCP 59s prometheus-kube-prometheus-operator ClusterIP 10.100.118.126 \u0026lt;none\u0026gt; 443/TCP 59s prometheus-kube-prometheus-prometheus LoadBalancer 10.100.19.126 a9a5daa0ee24243dabd0d771f77df7c7-832896403.us-east-1.elb.amazonaws.com 9090:31784/TCP 59s prometheus-kube-state-metrics ClusterIP 10.100.254.140 \u0026lt;none\u0026gt; 8080/TCP 59s prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 55s prometheus-prometheus-node-exporter ClusterIP 10.100.8.65 \u0026lt;none\u0026gt; 9100/TCP 59s$ kubectl get pod -n prometheus NAME READY STATUS RESTARTS AGE alertmanager-prometheus-kube-prometheus-alertmanager-0 2/2 Running 0 86s prometheus-grafana-756d9b8485-769kp 2/2 Running 0 89s prometheus-kube-prometheus-operator-686b89b849-4v25b 1/1 Running 0 90s prometheus-kube-state-metrics-58c5cd6ddb-mhjpt 1/1 Running 0 90s prometheus-prometheus-kube-prometheus-prometheus-0 2/2 Running 0 86s prometheus-prometheus-node-exporter-8qddg 1/1 Running 0 90s Check Prometheus Get the Prometheus\u0026rsquo; Load Balancer address  $ kubectl get service prometheus-kube-prometheus-prometheus -n prometheus --output=jsonpath=\u0026lsquo;{.status.loadBalancer.ingress[0].hostname}\u0026rsquo; a9a5daa0ee24243dabd0d771f77df7c7-832896403.us-east-1.elb.amazonaws.com \nRedirect your browser to http://a9a5daa0ee24243dabd0d771f77df7c7-832896403.us-east-1.elb.amazonaws.com:9090\nCheck Grafana Do the same thing for Grafana  $ kubectl get service prometheus-grafana -n prometheus --output=jsonpath=\u0026lsquo;{.status.loadBalancer.ingress[0].hostname}\u0026rsquo; a5f6d918221a14383813bc2f6b88c6c4-2050279250.us-east-1.elb.amazonaws.com \nGet Grafana admin\u0026rsquo;s password:  $ kubectl get secret prometheus-grafana -n prometheus -o jsonpath=\u0026ldquo;{.data.admin-password}\u0026rdquo; | base64 \u0026ndash;decode ; echo prom-operator \nRedirect your browser to it: http://a5f6d918221a14383813bc2f6b88c6c4-2050279250.us-east-1.elb.amazonaws.com. Use the admin id with the password we got.\nKong Data Plane and Prometheus Service Monitor In order to monitor the Kong Data Planes replicas we\u0026rsquo;re going to configure a Service Monitor based on a Kubernetes Service created for the Data Planes. The diagram shows the topology:\nCreate a Global Prometheus plugin First of all we have to configure the specific Prometheus plugin provided by Kong. After submitting the following declaration, all Ingresses defined will have the plugin enabled and, therefore, include their metrics on the Prometheus endpoint exposed by the Kong Data Plane.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongClusterPlugin metadata: name: prometheus annotations: kubernetes.io/ingress.class: kong labels: global: \u0026#34;true\u0026#34; plugin: prometheus EOF Expose the Data Plane metrics endpoint with a Kubernetes Service The next thing to do is to expose the Data Plane metrics port as a new Kubernetes Service. The new Kubernetes Service will be consumed by the Prometheus Service Monitor we\u0026rsquo;re going to configure later.\nThe new Kubernetes Service will be based on the metrics port 8100 provided by the Data Plane. We set the port during the Data Plane installation using the parameter --set env.status_listen=0.0.0.0:8100. You can check the port running:\n$ kubectl describe pod kong-dp-kong -n kong-dp | grep Ports Ports: 8000/TCP, 8443/TCP, 8100/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP Here\u0026rsquo;s the new Kubernetes Service declaration:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: kong-dp-monitoring namespace: kong-dp labels: app: kong-dp-monitoring spec: selector: app.kubernetes.io/name: kong type: ClusterIP ports: - name: metrics protocol: TCP port: 8100 targetPort: 8100 EOF Note that the new Kubernetes Service is selecting the existing Data Plane Kubernetes Service using its specific label app.kubernetes.io/name: kong\nUse can check the label running:\n$ kubectl get service -n kong-dp -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 38m app.kubernetes.io/component=app,app.kubernetes.io/instance=kong-dp,app.kubernetes.io/name=kong kong-dp-monitoring ClusterIP 10.100.170.163 \u0026lt;none\u0026gt; 8100/TCP 6s app.kubernetes.io/name=kong After submitting the declaration you should see the new Kubernetes Service:\n$ kubectl get service -n kong-dp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 38m kong-dp-monitoring ClusterIP 10.100.170.163 \u0026lt;none\u0026gt; 8100/TCP 34s Test the service. On one local terminal, expose the port 8100 using port-forward\n$ kubectl port-forward service/kong-dp-monitoring -n kong-dp 8100 Forwarding from 127.0.0.1:8100 -\u0026gt; 8100 Forwarding from [::1]:8100 -\u0026gt; 8100 On another terminal send a request to it:\n $ http :8100/metrics HTTP/1.1 200 OK Access-Control-Allow-Origin: * Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Thu, 30 Sep 2021 15:25:09 GMT Server: kong/2.5.1.0-enterprise-edition Transfer-Encoding: chunked X-Kong-Admin-Latency: 4 X-Kong-Status-Request-ID: QNX72r2DlUxloZaqoWkMIBBU0Rvug2Jg # HELP kong_datastore_reachable Datastore reachable from Kong, 0 is unreachable # TYPE kong_datastore_reachable gauge kong_datastore_reachable 1 # HELP kong_enterprise_license_expiration Unix epoch time when the license expires, the timestamp is substracted by 24 hours to avoid difference in timezone # TYPE kong_enterprise_license_expiration gauge kong_enterprise_license_expiration 1653998400 # HELP kong_enterprise_license_features License features features # TYPE kong_enterprise_license_features gauge kong_enterprise_license_features{feature=\"ee_plugins\"} 1 kong_enterprise_license_features{feature=\"write_admin_api\"} 1 # HELP kong_enterprise_license_signature Last 32 bytes of the license signautre in number # TYPE kong_enterprise_license_signature gauge kong_enterprise_license_signature 3.5512300986528e+40 # HELP kong_memory_lua_shared_dict_bytes Allocated slabs in bytes in a shared_dict # TYPE kong_memory_lua_shared_dict_bytes gauge kong_memory_lua_shared_dict_bytes{shared_dict=\"kong\",kong_subsystem=\"http\"} 40960 ………….  Create the Prometheus Service Monitor Now, let\u0026rsquo;s create the Prometheus Service Monitor collecting metrics from all Data Planes instances. The Service Monitor is based on the new kong-dp-monitoring Kubernetes Service we created before:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: kong-dp-service-monitor namespace: kong-dp labels: release: kong-dp spec: namespaceSelector: any: true endpoints: - port: metrics selector: matchLabels: app: kong-dp-monitoring EOF Starting a Prometheus instance for the Kong Data Plane A specific Prometheus instance will be created to monitor the Kong Data Plane using a specific \u0026ldquo;kong-prometheus\u0026rdquo; account. Before doing it, we need to create the account and grant specific permissions.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: ServiceAccount metadata: name: kong-prometheus namespace: kong-dp EOFcat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: prometheus rules: - apiGroups: [\u0026#34;\u0026#34;] resources: - nodes - nodes/metrics - services - endpoints - pods verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: - configmaps verbs: [\u0026#34;get\u0026#34;] - apiGroups: - networking.k8s.io resources: - ingresses verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] EOFcat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: prometheus roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus subjects: - kind: ServiceAccount name: kong-prometheus namespace: kong-dp EOF Instantiate a Prometheus Service for the Kong Data Plane\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: monitoring.coreos.com/v1 kind: Prometheus metadata: name: kong-dp-prometheus namespace: kong-dp spec: serviceAccountName: kong-prometheus serviceMonitorSelector: matchLabels: release: kong-dp resources: requests: memory: 400Mi enableAdminAPI: true EOF Check the Installation\n$ kubectl get pod -n kong-dp NAME READY STATUS RESTARTS AGE kong-dp-kong-67995ddc8c-4vhph 1/1 Running 0 13m prometheus-kong-dp-prometheus-0 2/2 Running 0 43s$ kubectl get service -n kong-dp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 44m kong-dp-monitoring ClusterIP 10.100.170.163 \u0026lt;none\u0026gt; 8100/TCP 6m38s prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 4m12s$ kubectl get prometheus -n kong-dp NAME VERSION REPLICAS AGE kong-dp-prometheus 4m8s Expose the new Prometheus service\n$ kubectl expose service prometheus-operated --name prometheus-operated-lb --type=LoadBalancer -n kong-dp service/prometheus-operated-lb exposed$ kubectl get service -n kong-dp NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kong-dp-kong-proxy LoadBalancer 10.100.56.103 a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com 80:31032/TCP,443:30651/TCP 45m kong-dp-monitoring ClusterIP 10.100.170.163 \u0026lt;none\u0026gt; 8100/TCP 7m20s prometheus-operated ClusterIP None \u0026lt;none\u0026gt; 9090/TCP 4m54s prometheus-operated-lb LoadBalancer 10.100.77.90 a81a086ab48e446d68c35d0bd0e93550-437150660.us-east-1.elb.amazonaws.com 9090:31798/TCP 14s"
},
{
	"uri": "/060_kong_ingress/063_rate_limiting.html",
	"title": "Rate Limiting Policy Definition",
	"tags": [],
	"description": "",
	"content": " Rate Limiting Policy Definition Now let\u0026rsquo;s protect our upstream service with a Rate Limiting policy.\nCreate the plugin\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: rl-by-minute namespace: default config: minute: 3 policy: local plugin: rate-limiting EOF If you want to delete it run:\n$ kubectl delete kongplugin rl-by-minute Apply the plugin to the route\nkubectl patch ingress sampleroute -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;konghq.com/plugins\u0026#34;:\u0026#34;proxycache, rl-by-minute\u0026#34;}}}\u0026#39; In case you want to disapply the plugin to the ingress run:\n$ kubectl annotate ingress sampleroute konghq.com/plugins- Test the plugin\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Age: 6 Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 20:53:58 GMT RateLimit-Limit: 3 RateLimit-Remaining: 2 RateLimit-Reset: 2 Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Cache-Key: f2d45950abe49485a51167bb1d1deae0 X-Cache-Status: Hit X-Kong-Proxy-Latency: 1 X-Kong-Upstream-Latency: 0 X-RateLimit-Limit-Minute: 3 X-RateLimit-Remaining-Minute: 2 Hello World, Kong: 2021-07-08 20:53:58.071403 As expected, we get an error for the 4th request::\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 429 Too Many Requests Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Thu, 08 Jul 2021 20:54:09 GMT RateLimit-Limit: 3 RateLimit-Remaining: 0 RateLimit-Reset: 51 Retry-After: 51 Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 X-RateLimit-Limit-Minute: 3 X-RateLimit-Remaining-Minute: 0 { \u0026#34;message\u0026#34;: \u0026#34;API rate limit exceeded\u0026#34; }"
},
{
	"uri": "/060_kong_ingress/064_api_key.html",
	"title": "API Key Policy Definition",
	"tags": [],
	"description": "",
	"content": " API Key Policy Definition Now, let\u0026rsquo;s add an API Key Policy to this route:\nCreate the plugin\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongPlugin metadata: name: apikey namespace: default plugin: key-auth EOF If you want to delete it run:\n$ kubectl delete kongplugin apikey Now, let\u0026rsquo;s add an API Key Policy to this route keeping the original Rate Limiting plugin:\nkubectl patch ingress sampleroute -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;konghq.com/plugins\u0026#34;:\u0026#34;proxycache, rl-by-minute, apikey\u0026#34;}}}\u0026#39; As expected, if we try to consume the route we get an error:\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 401 Unauthorized Connection: keep-alive Content-Length: 45 Content-Type: application/json; charset=utf-8 Date: Thu, 08 Jul 2021 20:56:08 GMT Server: kong/2.4.1.1-enterprise-edition WWW-Authenticate: Key realm=\u0026#34;kong\u0026#34; X-Kong-Response-Latency: 1 { \u0026#34;message\u0026#34;: \u0026#34;No API key found in request\u0026#34; } Provisioning a Key\n$ kubectl create secret generic consumerapikey --from-literal=kongCredType=key-auth --from-literal=key=kong-secret If you want to delete it run:\n$ kubectl delete secret consumerapikey Creating a Consumer with the Key\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongConsumer metadata: name: consumer1 namespace: default annotations: kubernetes.io/ingress.class: kong username: consumer1 credentials: - consumerapikey EOF If you want to delete it run:\n$ kubectl delete kongconsumer consumer1 Consume the route with the API Key\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello apikey:kong-secret HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 21:00:13 GMT RateLimit-Limit: 3 RateLimit-Remaining: 2 RateLimit-Reset: 47 Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Cache-Key: 5e5b92c154e1de64d2db0b245ce5a9ca X-Cache-Status: Miss X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 X-RateLimit-Limit-Minute: 3 X-RateLimit-Remaining-Minute: 2 Hello World, Kong: 2021-07-08 21:00:13.786471 Again, if we try the 4th request in a single minute we get the rate limiting error\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello apikey:kong-secret HTTP/1.1 429 Too Many Requests Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Thu, 08 Jul 2021 21:00:19 GMT RateLimit-Limit: 3 RateLimit-Remaining: 0 RateLimit-Reset: 41 Retry-After: 41 Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 X-RateLimit-Limit-Minute: 3 X-RateLimit-Remaining-Minute: 0 { \u0026#34;message\u0026#34;: \u0026#34;API rate limit exceeded\u0026#34; } Disable all plugins on the Ingress\n$ kubectl annotate ingress sampleroute konghq.com/plugins- You should be able to consume it with no API Key:\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 08 Jul 2021 21:01:28 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 Hello World, Kong: 2021-07-08 21:01:28.418190"
},
{
	"uri": "/040_kong_enterprise_setup/044_elk.html",
	"title": "ELK Stack",
	"tags": [],
	"description": "",
	"content": " ELK Stack From the Monitoring and Log Processing perspective, it\u0026rsquo;s important to integrate Kong Konnect Enterprise with a best-of-breed product to externalize all information related to processed requests and allow users to define dashboard, alerts, reports, etc.\nThis part of the tutorial shows how to configure the real-time integration between Kong Enterprise and Elastic products: Elasticsearch, Kibana and Logstash.\nElasticsearch Install Elasticsearch\n kubectl create namespace elk helm install elk elastic/elasticsearch -n elk --set replicas=1 --set minimumMasterNodes=1  Logstash Fetch the Charts and Update values.yaml file\nhelm fetch elastic/logstash tar xvfk logstash* cd logstash cp values.yaml logstash-values.yaml Update \u0026ldquo;logstashPipeline\u0026rdquo; with:\nlogstashPipeline: logstash.conf: | input { tcp { port =\u0026gt; 5044 codec =\u0026gt; \u0026#34;json\u0026#34; } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;http://elasticsearch-master.elk.svc.cluster.local:9200\u0026#34;] index =\u0026gt; \u0026#34;kong\u0026#34; } } Update \u0026ldquo;service\u0026rdquo; field with:\nservice: annotations: type: ClusterIP ports: - name: logstash port: 5044 protocol: TCP targetPort: 5044 Install Logstash\nhelm install logstash elastic/logstash -n elk -f logstash-values.yaml Kibana Install Kibana\nhelm install kibana elastic/kibana -n elk --set service.type=LoadBalancer Checking the installations\n$ kubectl get pod -n elk NAME READY STATUS RESTARTS AGE elasticsearch-master-0 1/1 Running 0 2m49s kibana-kibana-56689685dc-c25mq 1/1 Running 0 2m logstash-logstash-0 1/1 Running 0 2m19s$ kubectl get service -n elk NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-master ClusterIP 10.100.213.248 \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 2m53s elasticsearch-master-headless ClusterIP None \u0026lt;none\u0026gt; 9200/TCP,9300/TCP 2m53s kibana-kibana LoadBalancer 10.100.178.84 a39e2c2d48c044a67bc5f45e22c67b09-669447722.us-east-1.elb.amazonaws.com 5601:32072/TCP 2m3s logstash-logstash ClusterIP 10.100.122.2 \u0026lt;none\u0026gt; 5044/TCP 2m22s logstash-logstash-headless ClusterIP None \u0026lt;none\u0026gt; 9600/TCP 2m22s Setting the TCP-Log Plugin The externalization of all processed requests data to ELK is done by a TCP stream defined through the TCP-Log plugin.\nApply the TCP-Log plugin\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongClusterPlugin metadata: name: tcp-log annotations: kubernetes.io/ingress.class: kong labels: global: \u0026#34;true\u0026#34; config: host: logstash-logstash.elk.svc.cluster.local port: 5044 plugin: tcp-log EOF"
},
{
	"uri": "/040_kong_enterprise_setup.html",
	"title": "Kong Konnect Enterprise Set Up",
	"tags": [],
	"description": "",
	"content": " Setting up Kong Kong Gateway Enterprise can be used with or without a license, or on \u0026ldquo;Free Mode\u0026rdquo;. For Enterprise functionality, Kong Gateway enforces the presence and validity of a Kong Konnect license file.\nYou can check Kong Gateway licensing and \u0026ldquo;Free Mode\u0026rdquo; capabilities here: https://docs.konghq.com/enterprise/2.5.x/deployment/licensing/\nThis is a \u0026ldquo;Free Mode\u0026rdquo; deployment based on Kong Gateway Enterprise 2.5. Please contact Kong to get a Kong Enterprise trial license to use its Enterprise features.\n"
},
{
	"uri": "/040_kong_enterprise_setup/045_redis.html",
	"title": "Redis",
	"tags": [],
	"description": "",
	"content": " Redis Redis can be consumed by Kong Data Plane for two use cases:\n Caching: data coming from the upstream services can be cached to provide even better response and latence times Rate Limiting: to allow the multiple instances of the Kong Data Plane to process the same rate limiting counters  We\u0026rsquo;re going to explore Kong Data Plane and Redis integration in the next sections of the workshop.\nInstall Redis  kubectl create namespace redis  cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: redis namespace: redis labels: app: redis spec: selector: matchLabels: app: redis replicas: 1 template: metadata: labels: app: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 --- apiVersion: v1 kind: Service metadata: name: redis namespace: redis labels: app: redis spec: ports: - port: 6379 targetPort: 6379 selector: app: redis EOF Check the installation  $ kubectl get pod -n redis NAME READY STATUS RESTARTS AGE redis-fd794cd65-dpkht 1/1 Running 0 6s   $ kubectl get service -n redis NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis ClusterIP 10.100.250.164  6379/TCP 17s  "
},
{
	"uri": "/060_kong_ingress/065_ingress_rule.html",
	"title": "Set up Ingress rule for Kong Ingress",
	"tags": [],
	"description": "",
	"content": " Set up Ingress rule for Kong Ingress In this learning lab, you will learn how to use the KongIngress resource to control proxy behavior.\nDeploy the new Service cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: labels: app: echo name: echo spec: ports: - port: 8080 name: high protocol: TCP targetPort: 8080 - port: 80 name: low protocol: TCP targetPort: 8080 selector: app: echo --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: echo name: echo spec: replicas: 1 selector: matchLabels: app: echo strategy: {} template: metadata: creationTimestamp: null labels: app: echo spec: containers: - image: gcr.io/kubernetes-e2e-test-images/echoserver:2.2 name: echo ports: - containerPort: 8080 env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP resources: {} EOF Create the Ingress cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: demo annotations: kubernetes.io/ingress.class: kong spec: rules: - http: paths: - path: /foo backend: serviceName: echo servicePort: 80 EOF Consume the Ingress $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Thu, 08 Jul 2021 23:03:41 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 1 X-Kong-Upstream-Latency: 0 Hostname: echo-5fc5b5bc84-vhs5g Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-vhs5g pod namespace:\tdefault pod IP:\t192.168.20.213 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=GET real path=/foo query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/foo Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/foo x-forwarded-port=80 x-forwarded-proto=http x-real-ip=192.168.29.188 Request Body: -no body in request- You can try POST also\n$ http post a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Thu, 08 Jul 2021 23:03:52 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 1 X-Kong-Upstream-Latency: 0 Hostname: echo-5fc5b5bc84-r44tb Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-r44tb pod namespace:\tdefault pod IP:\t192.168.22.59 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=POST real path=/foo query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/foo Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive content-length=0 host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/foo x-forwarded-port=80 x-forwarded-proto=http x-real-ip=192.168.29.188 Request Body: -no body in request- Create the KongIngress rule Kong can strip the path defined in the ingress rule before proxying the request to the service. This can be seen in the real path=/ value in the response.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongIngress metadata: name: sample-customization route: methods: - GET strip_path: true EOF Associate ingress resource kubectl patch ingress demo -p \u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{\u0026#34;konghq.com/override\u0026#34;:\u0026#34;sample-customization\u0026#34;}}}\u0026#39; If you want to remove the annotation run:\nkubectl annotate ingress demo konghq.com/override- Now, Kong will proxy only GET requests on /foo/baz path and strip away /foo:\nConsume the Ingress again $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo/baz HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Thu, 08 Jul 2021 23:13:20 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 Hostname: echo-5fc5b5bc84-r44tb Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-r44tb pod namespace:\tdefault pod IP:\t192.168.22.59 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=GET real path=/baz query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/baz Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/foo/baz x-forwarded-port=80 x-forwarded-prefix=/foo x-forwarded-proto=http x-real-ip=192.168.29.188 Request Body: -no body in request- And if we send a POST, Kong will not route your Ingress\n$ http post a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo HTTP/1.1 404 Not Found Connection: keep-alive Content-Length: 48 Content-Type: application/json; charset=utf-8 Date: Thu, 08 Jul 2021 23:13:58 GMT Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 { \u0026#34;message\u0026#34;: \u0026#34;no Route matched with those values\u0026#34; } Delete the Ingress and Rule\nkubectl delete ingress demo kubectl delete kongingress sample-customization"
},
{
	"uri": "/060_kong_ingress/066_fallback_service.html",
	"title": "Configuring a fallback service",
	"tags": [],
	"description": "",
	"content": " Configuring a fallback service In this learning lab, you will learn how to setup a fallback service using Ingress resource. The fallback service will receive all requests that don\u0026rsquo;t match against any of the defined Ingress rules.\nThis can be useful for scenarios where you would like to return a 404 page to the end user if the user clicks on a dead link or inputs an incorrect URL.\nCreate the Ingress We\u0026rsquo;re going to create an Ingress for the Echo Service we deployed previously\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: demo annotations: konghq.com/strip-path: \u0026#34;true\u0026#34; kubernetes.io/ingress.class: kong spec: rules: - http: paths: - path: /cafe backend: serviceName: echo servicePort: 80 EOF Consume the Ingress $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/cafe/status/200 HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:00:59 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 Hostname: echo-5fc5b5bc84-vzskb Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-vzskb pod namespace:\tdefault pod IP:\t192.168.22.59 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=GET real path=/status/200 query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/status/200 Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/cafe/status/200 x-forwarded-port=80 x-forwarded-prefix=/cafe x-forwarded-proto=http x-real-ip=192.168.29.188 Request Body: -no body in request- Create a Fallback Service cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: fallback-svc spec: replicas: 1 selector: matchLabels: app: fallback-svc template: metadata: labels: app: fallback-svc spec: containers: - name: fallback-svc image: hashicorp/http-echo args: - \u0026#34;-text\u0026#34; - \u0026#34;This is not the path you are looking for. - Fallback service\u0026#34; ports: - containerPort: 5678 --- apiVersion: v1 kind: Service metadata: name: fallback-svc labels: app: fallback-svc spec: type: ClusterIP ports: - port: 80 targetPort: 5678 protocol: TCP name: http selector: app: fallback-svc EOF Create the KongIngress rule cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: fallback annotations: kubernetes.io/ingress.class: kong spec: backend: serviceName: fallback-svc servicePort: 80 EOF Consume the Ingress again $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/asdasd HTTP/1.1 200 OK Connection: keep-alive Content-Length: 61 Content-Type: text/plain; charset=utf-8 Date: Fri, 09 Jul 2021 12:06:54 GMT Via: kong/2.4.1.1-enterprise-edition X-App-Name: http-echo X-App-Version: 0.2.3 X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 0 This is not the path you are looking for. - Fallback service Delete the Ingress and Rule\nkubectl delete ingress demo fallback kubectl delete deployment fallback-svc kubectl delete service fallback-svc"
},
{
	"uri": "/040_kong_enterprise_setup/046_sample_application.html",
	"title": "Sample App Installation",
	"tags": [],
	"description": "",
	"content": " Sample App Installation The going to deploy a very basic application to our EKS Cluster and protect it with Kong for Kubernetes Ingress Controller. The app simply returns the current datetime.\nDeploy Sample App cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: sample namespace: default labels: app: sample spec: type: ClusterIP ports: - port: 5000 name: http selector: app: sample --- apiVersion: apps/v1 kind: Deployment metadata: name: sample namespace: default spec: replicas: 1 selector: matchLabels: app: sample template: metadata: labels: app: sample version: v1 spec: containers: - name: sample image: claudioacquaviva/sampleapp ports: - containerPort: 5000 EOF Check the Deployment\n$ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.100.0.1 \u0026lt;none\u0026gt; 443/TCP 16h sample ClusterIP 10.100.200.96 \u0026lt;none\u0026gt; 5000/TCP 5s$ kubectl get pods NAME READY STATUS RESTARTS AGE sample-76db6bb547-w5nkk 1/1 Running 0 20s Open a local terminal and expose the application with \u0026ldquo;port forward\u0026rdquo;:\n$ kubectl port-forward service/sample 5000 Forwarding from 127.0.0.1:5000 -\u0026gt; 5000 Forwarding from [::1]:5000 -\u0026gt; 5000 Open another local terminal and consume the app sending a request like this:\n$ http :5000/hello HTTP/1.0 200 OK Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 30 Sep 2021 15:40:40 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Hello World, Kong: 2021-09-30 15:40:40.739851 Type ˆC on the first terminal to stop the application exposure.\n"
},
{
	"uri": "/060_kong_ingress/067_https_redirect.html",
	"title": "HTTP/S Redirect",
	"tags": [],
	"description": "",
	"content": " HTTP/S Redirect This guide walks through how to configure Kong Ingress Controller to redirect HTTP request to HTTPS so that all communication from the external world to your APIs and micro services is encrypted.\nCreate the Ingress We\u0026rsquo;re going to create an Ingress for the Echo Service we deployed previously\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: demo-redirect annotations: konghq.com/strip-path: \u0026#34;true\u0026#34; kubernetes.io/ingress.class: kong spec: rules: - http: paths: - path: /foo-redirect backend: serviceName: echo servicePort: 80 EOF Consume the Ingress $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redirect/status/200 HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:13:52 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 4 X-Kong-Upstream-Latency: 0 Hostname: echo-5fc5b5bc84-vzskb Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-vzskb pod namespace:\tdefault pod IP:\t192.168.22.59 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=GET real path=/status/200 query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/status/200 Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/foo-redirect/status/200 x-forwarded-port=80 x-forwarded-prefix=/foo-redirect x-forwarded-proto=http x-real-ip=192.168.29.188 Request Body: -no body in request- Set up HTTP/S redirect cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongIngress metadata: name: demo-redirect route: protocols: - https https_redirect_status_code: 302 EOF Consume the Ingress again $ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redirect/headers HTTP/1.1 302 Moved Temporarily Connection: keep-alive Content-Length: 110 Content-Type: text/html Date: Fri, 09 Jul 2021 12:15:38 GMT Location: https://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redirect/headers Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;302 Found\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;302 Found\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; The results is a redirect - 302 Moved Temporarily - issued from Kong as expected.\nThe Location header will contain the URL you need to use for an HTTPS request.\nPlease note that this URL will be different depending on your installation method. You can also grab the IP address of the load balance fronting Kong and send a HTTPS request to test it.\nUse location header to access the service via HTTPS.\nRemember to replace the Location URL with then one above.\n$ http --verify=no https://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redirect/headers HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:19:17 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 0 Hostname: echo-5fc5b5bc84-vzskb Pod Information: node name:\tip-192-168-29-188.eu-central-1.compute.internal pod name:\techo-5fc5b5bc84-vzskb pod namespace:\tdefault pod IP:\t192.168.22.59 Server values: server_version=nginx: 1.12.2 - lua: 10010 Request Information: client_address=192.168.4.240 method=GET real path=/headers query= request_version=1.1 request_scheme=http request_uri=http://a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com:8080/headers Request Headers: accept=*/* accept-encoding=gzip, deflate connection=keep-alive host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com user-agent=HTTPie/2.4.0 x-forwarded-for=192.168.29.188 x-forwarded-host=a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com x-forwarded-path=/foo-redirect/headers x-forwarded-port=443 x-forwarded-prefix=/foo-redirect x-forwarded-proto=https x-real-ip=192.168.29.188 Request Body: -no body in request- Delete the Ingress and Rule\nkubectl delete ingress demo-redirect kubectl delete kongingress demo-redirect"
},
{
	"uri": "/040_kong_enterprise_setup/047_simple_kong_ingress.html",
	"title": "Simple Kong Ingress",
	"tags": [],
	"description": "",
	"content": " Kong Ingress Create an Ingress CRDs In order to expose \u0026ldquo;sample\u0026rdquo; through K4K8S, we\u0026rsquo;re going to create a specific \u0026ldquo;/sampleroute\u0026rdquo; route. Initially, the route is totally open and can be consumed freely. The next sections enable, as the name suggests, an API Key and a Rate Limiting mechanisms to protect the route.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: sampleroute namespace: default annotations: konghq.com/strip-path: \u0026#34;true\u0026#34; kubernetes.io/ingress.class: kong spec: rules: - http: paths: - path: /sampleroute backend: serviceName: sample servicePort: 5000 EOF Consume the Ingress Sending a single request to the Data Plane to test the Ingress using the Kong Data Plane ELB:\n$ http a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello HTTP/1.1 200 OK Connection: keep-alive Content-Length: 45 Content-Type: text/html; charset=utf-8 Date: Thu, 30 Sep 2021 15:41:46 GMT Server: Werkzeug/1.0.1 Python/3.7.4 Via: kong/2.5.1.0-enterprise-edition X-Kong-Proxy-Latency: 1 X-Kong-Upstream-Latency: 1 Hello World, Kong: 2021-09-30 15:41:46.085222"
},
{
	"uri": "/060_kong_ingress/068_redis_ratelimiting.html",
	"title": "Redis for Rate Limiting",
	"tags": [],
	"description": "",
	"content": " Redis for Rate Limiting Kong can rate-limit your traffic without any external dependency. In such a case, Kong stores the request counters in-memory and each Kong node applies the rate-limiting policy independently. There is no synchronization of information being done in this case. But if Redis is available in your cluster, Kong can take advantage of it and synchronize the rate-limit information across multiple Kong nodes and enforce a slightly different rate-limiting policy.\nTurn HPA off First of all make sure you don\u0026rsquo;t have HPA set for the Data Plane. You can delete it using:\nkubectl delete hpa kong-dp-kong -n kong-dp Create the Ingress Again, we\u0026rsquo;re going to create an Ingress for the Echo Service we deployed previously\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: demo-redis annotations: konghq.com/strip-path: \u0026#34;true\u0026#34; kubernetes.io/ingress.class: kong spec: rules: - http: paths: - path: /foo-redis backend: serviceName: echo servicePort: 80 EOF Consume the Ingress $ http -h a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:24:51 GMT Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 1 Set up Rate Limiting without Redis As we did before, we are configuring Kong for Kubernetes to rate-limit traffic from any client to 5 requests per minute, applying this policy in a global sense. This means the rate-limit will apply across all services.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongClusterPlugin metadata: name: global-rate-limit annotations: kubernetes.io/ingress.class: kong labels: global: \u0026#34;true\u0026#34; config: minute: 5 policy: local plugin: rate-limiting EOF Consume the Ingress again As expected, the rate-limiting policy execution will return the specific response when processed multiple times:\n$ http -h a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:30:04 GMT RateLimit-Limit: 5 RateLimit-Remaining: 4 RateLimit-Reset: 56 Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 0 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 After 5 resquests:\n$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis HTTP/1.1 429 Too Many Requests Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Fri, 09 Jul 2021 12:30:13 GMT RateLimit-Limit: 5 RateLimit-Remaining: 0 RateLimit-Reset: 47 Retry-After: 47 Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 { \u0026#34;message\u0026#34;: \u0026#34;API rate limit exceeded\u0026#34; } Scale the Data Plane Now, let\u0026rsquo;s scale up the Kong Data Plane deployment to 3 pods, for scale ability and redundancy:\nkubectl scale --replicas 3 deployment kong-dp-kong -n kong-dp You can check the pods with:\n$ kubectl get pod -n kong-dp NAME READY STATUS RESTARTS AGE kong-dp-kong-67c5c7d4c5-9p8jv 1/1 Running 0 10s kong-dp-kong-67c5c7d4c5-fd4xd 1/1 Running 0 16h kong-dp-kong-67c5c7d4c5-ff4zs 1/1 Running 0 10s prometheus-kong-dp-prometheus-0 2/2 Running 1 19h Consume the Ingress again You will observe that the rate-limit is not consistent anymore and you can make more than 5 requests in a minute.\nTo understand this behavior, we need to understand how we have configured Kong. In the current policy, each Kong node is tracking a rate-limit in-memory and it will allow 5 requests to go through for a client. There is no synchronization of the rate-limit information across Kong nodes. In use-cases where rate-limiting is used as a protection mechanism and to avoid over-loading your services, each Kong node tracking it\u0026rsquo;s own counter for requests is good enough as a malicious user will hit rate-limits on all nodes eventually. Or if the load-balance in-front of Kong is performing some sort of deterministic hashing of requests such that the same Kong node always receives the requests from a client, then we won\u0026rsquo;t have this problem at all.\n$ http -h a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis/headers HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:39:42 GMT RateLimit-Limit: 5 RateLimit-Remaining: 3 RateLimit-Reset: 18 Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 0 X-Kong-Upstream-Latency: 0 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 3 $ http -h a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis/headers HTTP/1.1 200 OK Connection: keep-alive Content-Type: text/plain; charset=UTF-8 Date: Fri, 09 Jul 2021 12:39:43 GMT RateLimit-Limit: 5 RateLimit-Remaining: 4 RateLimit-Reset: 17 Server: echoserver Transfer-Encoding: chunked Via: kong/2.4.1.1-enterprise-edition X-Kong-Proxy-Latency: 5 X-Kong-Upstream-Latency: 0 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 4 Update the Kong Plugin to use Redis Note we\u0026rsquo;re using the Redis Kubernetes Service FQDN for the Redis policy:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: configuration.konghq.com/v1 kind: KongClusterPlugin metadata: name: global-rate-limit annotations: kubernetes.io/ingress.class: kong labels: global: \u0026#34;true\u0026#34; config: minute: 5 policy: redis redis_host: redis.redis.svc.cluster.local plugin: rate-limiting EOF$ http a6bf3f71a14a64dba850480616af8fc9-1188819016.eu-central-1.elb.amazonaws.com/foo-redis/headers HTTP/1.1 429 Too Many Requests Connection: keep-alive Content-Length: 41 Content-Type: application/json; charset=utf-8 Date: Fri, 09 Jul 2021 14:03:26 GMT RateLimit-Limit: 5 RateLimit-Remaining: 0 RateLimit-Reset: 34 Retry-After: 34 Server: kong/2.4.1.1-enterprise-edition X-Kong-Response-Latency: 1 X-RateLimit-Limit-Minute: 5 X-RateLimit-Remaining-Minute: 0 { \u0026#34;message\u0026#34;: \u0026#34;API rate limit exceeded\u0026#34; } Delete the Ingress and Rule\nkubectl delete ingress demo-redis kubectl delete kongclusterplugin global-rate-limit"
},
{
	"uri": "/010_introduction/10_foreword.html",
	"title": "Foreword",
	"tags": [],
	"description": "",
	"content": " Kong Konnect Kong Konnect is a service connectivity platform that provides technology teams at multi-cloud and hybrid organizations the “architectural freedom” to build APIs and services anywhere. Kong’s service connectivity platform provides a ﬂexible, technology-agnostic platform that supports any cloud, platform, protocol and architecture. Kong Enterprise supports the full lifecycle of service management, enabling users to easily design, test, secure, deploy, monitor, monetize and version their APIs. Over 250 enterprises trust Kong Enterprise to reduce time to market by bringing applications and services to market faster, as well as increase infrastructure ROI by maximizing resource efficiency through improved automation and reduced overhead.\n Decentralize Applications and Services - Break down monoliths into services or build new applications with distributed architectures to accelerate your journey to microservices, Kubernetes and service mesh. Secure and Govern APIs and Services - Gain visibility across all your services and empower your application teams to provide consistent security, governance, and compliance across APIs and services. Create a Developer Platform - Fuel your innovation engine by providing an internal developer platform built for distributed architectures. Featuring end-to-end automation, streamlined developer onboarding, and native integrations with containers and Kubernetes, enable your teams to rapidly design, publish and consume APIs and services. Use your external Developer Portal to fuel consumption of APIs by third-party developers, build an ecosystem and drive revenue.  "
},
{
	"uri": "/030_aws_setup_for_hosting_kong/30_provisiong_eks_infrastructure.html",
	"title": "Provisioning EKS infrastructure",
	"tags": [],
	"description": "",
	"content": " Creating an EKS Cluster Before getting started with the workshop, we recommend to have an EKS Cluster already available. If you don\u0026rsquo;t have one, please, follow these instructions.\nCreating an EKS Cluster We\u0026rsquo;re going to use eksctl to create our EKS Cluster.\n $ eksctl create cluster --name K4K8S --version 1.21 --region us-east-1 --without-nodegroup 2021-09-29 20:23:41 [ℹ] eksctl version 0.67.0 2021-09-29 20:23:41 [ℹ] using region us-east-1 2021-09-29 20:23:43 [ℹ] setting availability zones to [us-east-1f us-east-1c] 2021-09-29 20:23:43 [ℹ] subnets for us-east-1f - public:192.168.0.0/19 private:192.168.64.0/19 2021-09-29 20:23:43 [ℹ] subnets for us-east-1c - public:192.168.32.0/19 private:192.168.96.0/19 2021-09-29 20:23:43 [ℹ] using Kubernetes version 1.21 2021-09-29 20:23:43 [ℹ] creating EKS cluster \"K4K8S\" in \"us-east-1\" region with 2021-09-29 20:23:43 [ℹ] if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=K4K8S' 2021-09-29 20:23:43 [ℹ] CloudWatch logging will not be enabled for cluster \"K4K8S\" in \"us-east-1\" 2021-09-29 20:23:43 [ℹ] you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=K4K8S' 2021-09-29 20:23:43 [ℹ] Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster \"K4K8S\" in \"us-east-1\" 2021-09-29 20:23:43 [ℹ] 2 sequential tasks: { create cluster control plane \"K4K8S\", 2 sequential sub-tasks: { wait for control plane to become ready, 1 task: { create addons } } } 2021-09-29 20:23:43 [ℹ] building cluster stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:23:45 [ℹ] deploying stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:24:15 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:24:45 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:25:46 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:26:47 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:27:48 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:28:48 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:29:49 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:30:50 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:31:51 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:32:51 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:33:52 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:34:53 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:35:54 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:36:54 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:37:56 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:38:57 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-cluster\" 2021-09-29 20:43:05 [ℹ] waiting for the control plane availability... 2021-09-29 20:43:05 [✔] saved kubeconfig as \"/Users/claudio/.kube/config\" 2021-09-29 20:43:05 [ℹ] no tasks 2021-09-29 20:43:05 [✔] all EKS cluster resources for \"K4K8S\" have been created 2021-09-29 20:45:11 [ℹ] kubectl command should work with \"/Users/claudio/.kube/config\", try 'kubectl get nodes' 2021-09-29 20:45:11 [✔] EKS cluster \"K4K8S\" in \"us-east-1\" region is ready   $ eksctl get cluster 2021-09-29 20:46:49 [ℹ] eksctl version 0.67.0 2021-09-29 20:46:49 [ℹ] using region us-east-1 NAME REGION EKSCTL CREATED K4K8S us-east-1 True  Creating the NodeGroup Now let\u0026rsquo;s add a NodeGroup in our Cluster.\n $ kubectl get node No resources found   $ eksctl create nodegroup --cluster K4K8S --name K4K8S-node --region us-east-1 --node-type m5.2xlarge --nodes 1 --max-pods-per-node 50 2021-09-29 20:47:28 [ℹ] eksctl version 0.67.0 2021-09-29 20:47:28 [ℹ] using region us-east-1 2021-09-29 20:47:28 [ℹ] will use version 1.21 for new nodegroup(s) based on control plane version 2021-09-29 20:47:33 [ℹ] nodegroup \"K4K8S-node\" will use \"\" [AmazonLinux2/1.21] 2021-09-29 20:47:34 [ℹ] 1 nodegroup (K4K8S-node) was included (based on the include/exclude rules) 2021-09-29 20:47:34 [ℹ] will create a CloudFormation stack for each of 1 managed nodegroups in cluster \"K4K8S\" 2021-09-29 20:47:35 [ℹ] 2 sequential tasks: { fix cluster compatibility, 1 task: { 1 task: { create managed nodegroup \"K4K8S-node\" } } } 2021-09-29 20:47:35 [ℹ] checking cluster stack for missing resources 2021-09-29 20:47:35 [ℹ] cluster stack has all required resources 2021-09-29 20:47:35 [ℹ] building managed nodegroup stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:47:36 [ℹ] deploying stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:47:36 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:47:52 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:48:10 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:48:30 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:48:48 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:49:09 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:49:29 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:49:49 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:50:42 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:51:00 [ℹ] waiting for CloudFormation stack \"eksctl-K4K8S-nodegroup-K4K8S-node\" 2021-09-29 20:51:01 [ℹ] no tasks 2021-09-29 20:51:01 [✔] created 0 nodegroup(s) in cluster \"K4K8S\" 2021-09-29 20:51:02 [ℹ] nodegroup \"K4K8S-node\" has 1 node(s) 2021-09-29 20:51:02 [ℹ] node \"ip-192-168-42-135.ec2.internal\" is ready 2021-09-29 20:51:02 [ℹ] waiting for at least 1 node(s) to become ready in \"K4K8S-node\" 2021-09-29 20:51:02 [ℹ] nodegroup \"K4K8S-node\" has 1 node(s) 2021-09-29 20:51:02 [ℹ] node \"ip-192-168-42-135.ec2.internal\" is ready 2021-09-29 20:51:02 [✔] created 1 managed nodegroup(s) in cluster \"K4K8S\" 2021-09-29 20:51:03 [ℹ] checking security group configuration for all nodegroups 2021-09-29 20:51:03 [ℹ] all nodegroups have up-to-date configuration   $ kubectl get node NAME STATUS ROLES AGE VERSION ip-192-168-42-135.ec2.internal Ready  93s v1.21.2-eks-55daa9d   $ kubectl describe node Name: ip-192-168-42-135.ec2.internal Roles:  Labels: alpha.eksctl.io/cluster-name=K4K8S alpha.eksctl.io/nodegroup-name=K4K8S-node beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=m5.2xlarge beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=K4K8S-node eks.amazonaws.com/nodegroup-image=ami-0a99721a12001ebd4 eks.amazonaws.com/sourceLaunchTemplateId=lt-068b814fdb07dd19a eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=us-east-1 failure-domain.beta.kubernetes.io/zone=us-east-1c kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-192-168-42-135.ec2.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=m5.2xlarge topology.kubernetes.io/region=us-east-1 topology.kubernetes.io/zone=us-east-1c Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Wed, 29 Sep 2021 20:50:12 -0300 Taints:  Unschedulable: false Lease: HolderIdentity: ip-192-168-42-135.ec2.internal AcquireTime:  RenewTime: Wed, 29 Sep 2021 20:51:55 -0300 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Wed, 29 Sep 2021 20:51:12 -0300 Wed, 29 Sep 2021 20:50:10 -0300 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Wed, 29 Sep 2021 20:51:12 -0300 Wed, 29 Sep 2021 20:50:10 -0300 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Wed, 29 Sep 2021 20:51:12 -0300 Wed, 29 Sep 2021 20:50:10 -0300 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Wed, 29 Sep 2021 20:51:12 -0300 Wed, 29 Sep 2021 20:50:42 -0300 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.42.135 ExternalIP: 3.86.231.237 Hostname: ip-192-168-42-135.ec2.internal InternalDNS: ip-192-168-42-135.ec2.internal ExternalDNS: ec2-3-86-231-237.compute-1.amazonaws.com Capacity: attachable-volumes-aws-ebs: 25 cpu: 8 ephemeral-storage: 83873772Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 32067240Ki pods: 50 Allocatable: attachable-volumes-aws-ebs: 25 cpu: 7910m ephemeral-storage: 76224326324 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 31050408Ki pods: 50 System Info: Machine ID: ec28a5fbd4d7e76c7dd7e7b0c7ea1697 System UUID: ec28a5fb-d4d7-e76c-7dd7-e7b0c7ea1697 Boot ID: d93f6133-7a22-4a5e-a0ed-f2d2275ec303 Kernel Version: 5.4.141-67.229.amzn2.x86_64 OS Image: Amazon Linux 2 Operating System: linux Architecture: amd64 Container Runtime Version: docker://19.3.13 Kubelet Version: v1.21.2-eks-55daa9d Kube-Proxy Version: v1.21.2-eks-55daa9d ProviderID: aws:///us-east-1c/i-0a6956af83a16d483 Non-terminated Pods: (4 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- kube-system aws-node-kr2c4 10m (0%) 0 (0%) 0 (0%) 0 (0%) 108s kube-system coredns-66cb55d4f4-8pwrn 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 17m kube-system coredns-66cb55d4f4-hw4mq 100m (1%) 0 (0%) 70Mi (0%) 170Mi (0%) 17m kube-system kube-proxy-gb8bc 100m (1%) 0 (0%) 0 (0%) 0 (0%) 108s Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 310m (3%) 0 (0%) memory 140Mi (0%) 340Mi (1%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) attachable-volumes-aws-ebs 0 0 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 110s kubelet Starting kubelet. Normal NodeHasSufficientMemory 110s (x2 over 110s) kubelet Node ip-192-168-42-135.ec2.internal status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 110s (x2 over 110s) kubelet Node ip-192-168-42-135.ec2.internal status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 110s (x2 over 110s) kubelet Node ip-192-168-42-135.ec2.internal status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 110s kubelet Updated Node Allocatable limit across pods Normal Starting 101s kube-proxy Starting kube-proxy. Normal NodeReady 78s kubelet Node ip-192-168-42-135.ec2.internal status is now: NodeReady  Checking the EKS Cluster  $ kubectl get pod --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system aws-node-kr2c4 1/1 Running 0 2m20s kube-system coredns-66cb55d4f4-8pwrn 1/1 Running 0 18m kube-system coredns-66cb55d4f4-hw4mq 1/1 Running 0 18m kube-system kube-proxy-gb8bc 1/1 Running 0 2m20s   $ kubectl get service --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.100.0.1  443/TCP 18m kube-system kube-dns ClusterIP 10.100.0.10  53/UDP,53/TCP 18m  "
},
{
	"uri": "/010_introduction/12_problem_to_solve.html",
	"title": "The Challenge",
	"tags": [],
	"description": "",
	"content": " The Problem Every organization is under pressure to innovate by providing compelling digital experiences. Organizations overcome this challenge through applications that are built with modern architectures using microservices and containers, managed through Kubernetes. These modern methods deliver the agility needs of the businesses.\nOrganizations break down legacy monolithic applications into smaller modern services, or they build new cloud native apps using microservices. The number of APIs is exploding, and connectivity between services becomes the critical backbone of your applications.\nKong Konnect helps address these challenges by providing reliable, secure and observable connectivity for all services across any infrastructure. The platform provides full lifecycle API management to:\n Design, test and document APIs and microservices Connect services across any infrastructure Automate the API lifecycle  "
},
{
	"uri": "/010_introduction/14_partner.html",
	"title": "What is Kong?",
	"tags": [],
	"description": "",
	"content": " Kong Konnect Enterprise The Service Connectivity Platform End-to-end service connectivity The platform integrates an API gateway, service mesh and ingress to provide end-to-end service connectivity across the full API lifecycle. It is the only platform that provides end-to-end connectivity between services within applications and across applications as well as exposes services at the edge.\nArchitectural freedom You can abstract network and security concerns from services so that developers can focus on building apps instead of pipes. Kong connects services across any cloud, any platform and any modern protocol such as REST, GraphQL, gRPC and Kafka.\nHybrid and multi-cloud You want to deploy services in cloud computing resources closer to the physical business locations for performance, cost, privacy and regulatory compliance reasons. With Kong Konnect you can deploy APIs in a 100% consistent manner across any infrastructure and any deployment pattern such as hybrid or multi-cloud configurations.\nKubernetes native You can deploy Konnect in Kubernetes via the Kong Kubernetes Ingress Controller to manage microservices and APIs natively through Kubernetes CRDs. You can automate deployment tasks and ensure consistency of configurations with GitOps.\nExtensible Konnect’s core is fully extensible, and you can build your own custom plugins to complement the many out-of-box policies that are available with the open source or enterprise edition.\nFast, scalable and secure Konnect runtimes are lightweight, fast by design and remove bloat from each service. You can seamlessly scale up and down services and secure them with policies that enforce zero-trust security.\nAutomation Konnect enables declarative configuration by generating a configuration file for Kong, via Kubernetes Ingress or the Kong Gateway, and pushing it into a CI/CD pipeline via GitSync to provide a single version of the truth and to optimize DevOps and GitOps practices\nKong Konnect Components Kong Konnect includes a management plane and runtimes. The SaaS-based management plane accelerates deployments and provides access to functionality modules:\nKong Service Hub Build a comprehensive catalog of heterogeneous services to enable service consumability.\nKong Vitals Get a central view of all services and runtimes with real-time analytics.\nKong Runtime Manager Spin up and manage Kong Gateway across hybrid and multi-cloud, Kubernetes and self-hosted infrastructures with simplified and automated day-2 automation.\nKong Developer Portal Provide APIs as a product to external teams to consume through a customized portal.\nInsomnia Use spec-driven development to design, test and document APIs.\nKong Immunity Detect traffic anomalies to predict service disruptions and security vulnerabilities.\nKong Gateway The world’s most popular API gateway, optimized for microservices and distributed architectures.\nKong Mesh Universal service mesh for enterprise organizations focused on simplicity and scalability with Kuma and Envoy.\n"
},
{
	"uri": "/010_introduction/16_workshop_prerequisites.html",
	"title": "Workshop Prerequisites",
	"tags": [],
	"description": "",
	"content": " Workshop Prerequisites There are a few prerequisite tasks you must perform before getting started on this workshop. These are:\n access to an AWS account with proper permissions. eksctl kubectl\n httpie and curl jq fortio  In the next section, you will be instructed on how to configure all the prerequisite task stated above.\n"
},
{
	"uri": "/010_introduction/18_getting_started.html",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": " Getting Started Kong Gateway Enterprise is Kong’s API gateway with enterprise functionality. As part of Kong Konnect, the gateway brokers an organization’s information across all services by allowing customers to manage the full lifecycle of services and APIs. On top of that, it enables users to simplify the management of APIs and microservices across hybrid-cloud and multi-cloud deployments.\nKong Gateway is designed to run on decentralized architectures, leveraging workflow automation and modern GitOps practices. With Kong Gateway, users can:\n Decentralize applications/services and transition to microservices Create a thriving API developer ecosystem Proactively identify API-related anomalies and threats Secure and govern APIs/services, and improve API visibility across the entire organization  Kong Gateway is a combination of several features and modules built on top of the open-sourced Kong Gateway, as shown in the diagram and described in the next section, Kong Gateway Enterprise Features.\nKong Gateway Enterprise Features Kong Gateway Enterprise features are described in this section, including modules and plugins that extend and enhance the functionality of the Kong Konnect platform.\nKong Gateway (OSS) Kong Gateway (OSS) is a lightweight, fast, and flexible cloud-native API gateway. It’s easy to download, install, and configure to get up and running once you know the basics. The gateway runs in front of any RESTful API and is extended through modules and plugins which provide extra functionality beyond the core platform.\nKong Admin API Kong Admin API provides a RESTful interface for administration and configuration of Services, Routes, Plugins, and Consumers. All of the tasks you perform in the Kong Manager can be automated using the Kong Admin API. For more information, see Kong Admin API.\nKong Developer Portal Kong Developer Portal (Kong Dev Portal) is used to onboard new developers and to generate API documentation, create custom pages, manage API versions, and secure developer access. For more information, see Kong Developer Portal.\nKong Immunity Kong Immunity uses machine learning to autonomously identify service behavior anomalies in real-time to improve security, mitigate breaches and isolate issues. Use Kong Immunity to autonomously identify service issues with machine learning-powered anomaly detection. For more information, see Kong Immunity.\nKubernetes Ingress Controller Kong for Kubernetes Enterprise (K4K8S) is a Kubernetes Ingress Controller. A Kubernetes Ingress Controller is a proxy that exposes Kubernetes services from applications (for example, Deployments, ReplicaSets) running on a Kubernetes cluster to client applications running outside of the cluster. The intent of an Ingress Controller is to provide a single point of control for all incoming traffic into the Kubernetes cluster. For more information, see Kong for Kubernetes.\nKong Manager Kong Manager is the Graphical User Interface (GUI) for Kong Gateway Enterprise. It uses the Kong Admin API under the hood to administer and control Kong Gateway (OSS). Use Kong Manager to organize teams, adjust policies, and monitor performance with just a few clicks. Group your teams, services, plugins, consumer management, and more exactly how you want them. Create new routes and services, activate or deactivate plugins in seconds. For more information, see the Kong Manager Guide.\nKong Plugins Kong Gateway plugins provide advanced functionality to better manage your API and microservices. With turnkey capabilities to meet the most challenging use cases, Kong Gateway Enterprise plugins ensure maximum control and minimizes unnecessary overhead. Enable features like authentication, rate-limiting, and transformations by enabling Kong Gateway Enterprise plugins through Kong Manager or the Admin API. For more information on which plugins are Enterprise-only, see the Kong Hub.\nKong Vitals Kong Vitals provides useful metrics about the health and performance of your Kong Gateway Enterprise nodes, as well as metrics about the usage of your gateway-proxied APIs. You can visually monitor vital signs and pinpoint anomalies in real-time, and use visual API analytics to see exactly how your APIs and Gateway are performing and access key statistics. Kong Vitals is part of the Kong Manager UI. For more information, see Kong Vitals.\nInsomnia Insomnia enables spec-first development for all REST and GraphQL services. With Insomnia, organizations can accelerate design and test workflows using automated testing, direct Git sync, and inspection of all response types. Teams of all sizes can use Insomnia to increase development velocity, reduce deployment risk, and increase collaboration. For more information, see Insomnia documentation.\nKey Concepts and Terminology Kong Gateway Enterprise uses common terms for entities and processes that have a specific meaning in context. This topic provides a conceptual overview of terms, and how they apply to Kong’s use cases.\nAdmin An Admin is a Kong Gateway user account capable of accessing the Admin API or Kong Manager. With RBAC and Workspaces, access can be modified and limited to specific entities.\nAuthentication Authentication is the process by which a system validates the identity of a user account. It is a separate concept from authorization.\nAPI gateway authentication is an important way to control the data that is allowed to be transmitted to and from your APIs. An API may have a restricted list of identities that are authorized to access it. Authentication is the process of proving an identity.\nAuthorization Authorization is the system of defining access to certain resources. In Kong Gateway, Role-Based Access Control (RBAC) is the main authorization mode. To define authorization to an API, it is possible to use the ACL Plugin in conjunction with an authentication plugin.\nClient A Kong Client refers to the downstream client making requests to Kong’s proxy port. It could be another service in a distributed application, a user’s identity, a user’s browser, or a specific device.\nConsumer A Consumer object represents a client of a Service.\nA Consumer is also the Admin API entity representing a developer or machine using the API. When using Kong, a Consumer only communicates with Kong which proxies every call to the said upstream API.\nYou can either rely on Kong as the primary datastore, or you can map the consumer list with your database to keep consistency between Kong and your existing primary datastore.\nHost A Host represents the domain hosts (using DNS) intended to receive upstream traffic. In Kong, it is a list of domain names that match a Route object.\nMethods Methods represent the HTTP methods available for requests. It accepts multiple values, for example, GET, POST, and DELETE. Its default value is empty (the HTTP method is not used for routing).\nPermission A Permission is a policy representing the ability to create, read, update, or destroy an Admin API entity defined by endpoints.\nPlugin Plugins provide advanced functionality and extend the use of Kong Gateway, allowing you to add new features to your gateway. Plugins can be configured to run in a variety of contexts, ranging from a specific route to all upstreams. Plugins can perform operations in your environment, such as authentication, rate-limiting, or transformations on a proxied request.\nProxy Kong is a reverse proxy that manages traffic between clients and hosts. As a gateway, Kong’s proxy functionality evaluates any incoming HTTP request against the Routes you have configured to find a matching one. If a given request matches the rules of a specific Route, Kong processes proxying the request. Because each Route is linked to a Service, Kong runs the plugins you have configured on your Route and its associated Service and then proxies the request upstream.\nProxy Caching One of the key benefits of using a reverse proxy is the ability to cache frequently-accessed content. The benefit is that upstream services do not need to waste computation on repeated requests.\nOne of the ways Kong delivers performance is through Proxy Caching, using the Proxy Cache Advanced Plugin. This plugin supports performance efficiency by providing the ability to cache responses based on requests, response codes and content type.\nKong receives a response from a service and stores it in the cache within a specific timeframe.\nFor future requests within the timeframe, Kong responds from the cache instead of the service.\nThe cache timeout is configurable. Once the time expires, Kong forwards the request to the upstream again, caches the result, and then responds from the cache until the next timeout.\nThe plugin can store cached data in-memory. The tradeoff is that it competes for memory with other processes, so for improved performance, use Redis for caching.\nRate Limiting Rate Limiting allows you to restrict how many requests your upstream services receive from your API consumers, or how often each user can call the API. Rate limiting protects the APIs from inadvertent or malicious overuse. Without rate limiting, each user may request as often as they like, which can lead to spikes of requests that starve other consumers. After rate limiting is enabled, API calls are limited to a fixed number of requests per second.\nIn this workflow, we are going to enable the Rate Limiting Advanced Plugin. This plugin provides support for the sliding window algorithm to prevent the API from being overloaded near the window boundaries and adds Redis support for greater performance.\nRole A Role is a set of permissions that may be reused and assigned to Admins. For example, this diagram shows multiple admins assigned to a single shared role that defines permissions for a set of objects in a workspace.\nRoute A Route, also referred to as Route object, defines rules to match client requests to upstream services. Each Route is associated with a Service, and a Service may have multiple Routes associated with it. Routes are entry-points in Kong and define rules to match client requests. Once a Route is matched, Kong proxies the request to its associated Service. See the Proxy Reference for a detailed explanation of how Kong proxies traffic.\nService A Service, also referred to as a Service object, is the upstream APIs and microservices Kong manages. Examples of Services include a data transformation microservice, a billing API, and so on. The main attribute of a Service is its URL (where Kong should proxy traffic to), which can be set as a single string or by specifying its protocol, host, port and path individually. The URL can be composed by specifying a single string or by specifying its protocol, host, port, and path individually.\nBefore you can start making requests against a Service, you need to add a Route to it. Routes specify how (and if) requests are sent to their Services after they reach Kong. A single Service can have many Routes. After configuring the Service and the Route, you’ll be able to make requests through Kong using them.\nSuper Admin A Super Admin, or any Role with read and write access to the /admins and /rbac endpoints, creates new Roles and customize Permissions. A Super Admin can:\n Invite and disable other Admin accounts Assign and revoke Roles to Admins Create new Roles with custom Permissions Create new Workspaces  Tags Tags are customer defined labels that let you manage, search for, and filter core entities using the ?tags querystring parameter. Each tag must be composed of one or more alphanumeric characters, _\\, -, . or ~. Most core entities can be tagged via their tags attribute, upon creation or edition.\nTeams Teams organize developers into working groups, implements policies across entire environments, and onboards new users while ensuring compliance. Role-Based Access Control (RBAC) and Workspaces allow users to assign administrative privileges and grant or limit access privileges to individual users and consumers, entire teams, and environments across the Kong platform.\nUpstream An Upstream object refers to your upstream API/service sitting behind Kong, to which client requests are forwarded. An Upstream object represents a virtual hostname and can be used to load balance incoming requests over multiple services (targets). For example, an Upstream named service.v1.xyz for a Service object whose host is service.v1.xyz. Requests for this Service object would be proxied to the targets defined within the upstream.\nWorkspaces Workspaces enable an organization to segment objects and admins into namespaces. The segmentation allows teams of admins sharing the same Kong cluster to adopt roles for interacting with specific objects. For example, one team (Team A) may be responsible for managing a particular service, whereas another team (Team B) may be responsible for managing another service.\nMany organizations have strict security requirements. For example, organizations need the ability to segregate the duties of an administrator to ensure that a mistake or malicious act by one administrator does not cause an outage.\nTry Kong Gateway Enterprise Kong Gateway is available in free mode. Download it and start testing out Gateway’s open source features with Kong Manager today.\nKong Gateway is also bundled with Kong Konnect. There are a few ways to test out the gateway’s Plus or Enterprise features:\nSign up for a free trial of Kong Konnect Plus. Try out Kong Gateway on Kubernetes using a live tutorial at https://www.konglabs.io/kubernetes/. If you are interested in evaluating Enterprise features locally, the Kong sales team manages evaluation licenses as part of a formal sales process. The best way to get started with the sales process is to request a demo and indicate your interest.\n"
},
{
	"uri": "/020_event_engine_setup/20_aws_event_engine.html",
	"title": "1. AWS Event Engine",
	"tags": [],
	"description": "",
	"content": " Attending an AWS hosted event To complete this workshop, you will be provided with an AWS account via the AWS Event Engine service. A team hash will be provided to you by event staff.\nIf you are currently logged in to an AWS Account, you can log out using this link\n Logging into Event Engine Dashboard  Connect to the portal by clicking the button or browsing to https://dashboard.eventengine.run/. The following screen shows up. Enter the provided hash in the text box. The button in the bottom right corner changes to Accept Terms \u0026amp; Login. Click on that button to continue.   Choose AWS Console, then Open AWS Console.   Use a single region for the duration of this workshop. This workshop supports the following regions:   us-east-1 (US East - N.Virginia)  Please select US East (N.Virginia) in the top right corner.\nThis account will expire at the end of the workshop and all the resources created will be automatically de-provisioned. You will not be able to access this account after today.\n "
},
{
	"uri": "/020_event_engine_setup/22_setup_eks.html",
	"title": "2. Set up EKS Cluster",
	"tags": [],
	"description": "",
	"content": " Set up the Elastic Kubernetes Service Cluster You can create your EKS Cluster using the eksctl cli or the AWS EKS console.\nFollow the instructions described here to have your Cluster up and running.\n"
},
{
	"uri": "/050_kong_ingress_consumption_monitoring.html",
	"title": "Kong Ingress Consumption and Monitoring",
	"tags": [],
	"description": "",
	"content": " Kong Konnect Observability and Log Processing  Kong Konnect Observability: monitor the Kong Data Plane Deployment using Prometheus and Grafana Kong Konnect Log Processing: analyse all processed requests using ELK Stack  "
},
{
	"uri": "/050_kong_ingress_consumption_monitoring/051_prometheus_grafana.html",
	"title": "Prometheus and Grafana",
	"tags": [],
	"description": "",
	"content": " Prometheus and Grafana Since we have Prometheus and Grafana already deployed where going to consume the Data Plane and monitor it from two perspectives:\n Kubernetes monitoring: monitor the Kong Data Plane Deployment in terms of resource consumption (CPU, memory and networking) Kong Data Plane monitoring: monitor the Kong Data Plane in terms of API consumption including number of processed requests, latency times, etc.  Kubernetes Monitoring and HPA Fortio Use Fortio to start injecting some request to the Data Plane\n$ fortio load -c 120 -qps 2000 -t 0 http://a946e3cab079a49a1b6661ab62d5585f-2135097986.us-east-1.elb.amazonaws.com/sampleroute/hello Check Grafana Direct your browser to Grafana again. Click on \u0026ldquo;Dashboards\u0026rdquo; and \u0026ldquo;Manage\u0026rdquo;. Choose the \u0026ldquo;Kubernetes/Computer Resources/Namespaces (Pods)\u0026rdquo; dashboard. Choose the \u0026ldquo;kong-dp\u0026rdquo; namespace.\nCheck HPA Since we\u0026rsquo;re using HPA, the number of Pods should increase to satify our settings.\nIn fact, we can see the new current HPA status with:\n$ kubectl get hpa -n kong-dp NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE kong-dp-kong Deployment/kong-dp-kong 15%/75% 1 20 3 15h Kong Data Plane Monitoring Consume the new Prometheus Service Get the API consumption rate\n$ curl -gs \u0026#39;http://a81a086ab48e446d68c35d0bd0e93550-437150660.us-east-1.elb.amazonaws.com:9090/api/v1/query?query=sum(rate(kong_http_status{code=\u0026#34;200\u0026#34;}[1m]))\u0026#39; | jq -r .data.result[].value[1] 199.8 Get the number of successful processed requests\n$ curl -gs \u0026#39;http://a81a086ab48e446d68c35d0bd0e93550-437150660.us-east-1.elb.amazonaws.com:9090/api/v1/query?query=kong_http_status{code=\u0026#34;200\u0026#34;}\u0026#39; | jq -r .data.result[].value[1] 669973 Check the new Prometheus instance GUI redirecting your browser to http://a81a086ab48e446d68c35d0bd0e93550-437150660.us-east-1.elb.amazonaws.com:9090\n$ kubectl get hpa -n kong-dp NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE kong-dp-kong Deployment/kong-dp-kong 66%/75% 1 20 7 80m $ kubectl get pod -n kong-dp NAME READY STATUS RESTARTS AGE kong-dp-kong-67995ddc8c-2t2gj 1/1 Running 0 2m44s kong-dp-kong-67995ddc8c-4vhph 1/1 Running 0 80m kong-dp-kong-67995ddc8c-4xjv2 1/1 Running 0 2m44s kong-dp-kong-67995ddc8c-7jsxl 1/1 Running 0 4m30s kong-dp-kong-67995ddc8c-8k44s 1/1 Running 0 3m44s kong-dp-kong-67995ddc8c-bpdvh 1/1 Running 0 2m44s kong-dp-kong-67995ddc8c-npnjf 1/1 Running 0 4m15s prometheus-kong-dp-prometheus-0 2/2 Running 0 67m Accessing Grafana Create a new Grafana Data Source based on the Prometheus Service URL: http://prometheus-operated.kong-dp.svc.cluster.local:9090\nNow, based on this new Data Source, import the official Kong Grafana Dashboard with id 7424\nYou should be able to see Kong Data Plane metrics now:\n"
},
{
	"uri": "/050_kong_ingress_consumption_monitoring/052_elk.html",
	"title": "ELK",
	"tags": [],
	"description": "",
	"content": " ELK Stack Now let\u0026rsquo;s use ELK Stack to receive and work with all requests coming from the Kong Data Planes\nCreate an ELK Index $ kubectl get service kibana-kibana --output=jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; -n elk a39e2c2d48c044a67bc5f45e22c67b09-669447722.us-east-1.elb.amazonaws.com Redirect your browser to the Load Balancer instantiated for it: http://a39e2c2d48c044a67bc5f45e22c67b09-669447722.us-east-1.elb.amazonaws.com:5601\nClick on Explore on my own On the left menu click on Management -\u0026gt; Stack Management\nClick on Data -\u0026gt; Index Management. Since we\u0026rsquo;re already send requests to the Data Plane and we have enabled the TCP-Log plugin, we should see the kong index as we set in our Logstash configuration.\nClick on Kibana -\u0026gt; Index Pattern -\u0026gt; Create index pattern. For the Index pattern name choose kong. Click on Next Step\nOn the Step 2 page choose @timestamp for Time Field. Click on Create index pattern\nWork with the Kong Data Plane requests On the left menu click on Analytics -\u0026gt; Discover. You should see the requests coming from the Kong Data Plane.\n"
},
{
	"uri": "/060_kong_ingress.html",
	"title": "Kong Ingress Controller Policies",
	"tags": [],
	"description": "",
	"content": " Kong Ingress Controller Policies Since we have an sample application deployed and an Ingress exposing it, it\u0026rsquo;s time to control such exposure.\nTo get started, we\u0026rsquo;re going to use three fundamental plugins provided by Kong:\n Proxy Caching plugin: it caches and serves commonly requested responses in Kong\n Rate Limiting plugin: it limits how many HTTP requests can be made in a given period of seconds, minutes, hours, days, months, or years. We\u0026rsquo;re going to define a basic 3-request a minute policy\n Key Authentication plugin: also sometimes referred to as an API key, controls the consumers sending requests to the Gateway.\n  Feel free to change the policies used and experiment further implementing policies like caching OIDC-based authentication, canary, GraphQL integration, and more with the extensive list of plugins provided by Kong. Check the list over here: https://docs.konghq.com/hub/\nThen we\u0026rsquo;re going to apply other policies to our previously created Ingress\n Set up Ingress rule for Kong Ingress\n Configuring a fallback service\n Configuring HTTPS redirect for services\n Using Redis for rate-limiting\n  "
},
{
	"uri": "/070_kong_cognito.html",
	"title": "Kong and AWS Cognito",
	"tags": [],
	"description": "",
	"content": " Kong Ingress Controller Policies Since we have an sample application deployed and an Ingress exposing it, it\u0026rsquo;s time to control such exposure.\nTo get started, we\u0026rsquo;re going to use three fundamental plugins provided by Kong:\n Proxy Caching plugin: it caches and serves commonly requested responses in Kong\n Rate Limiting plugin: it limits how many HTTP requests can be made in a given period of seconds, minutes, hours, days, months, or years. We\u0026rsquo;re going to define a basic 3-request a minute policy\n Key Authentication plugin: also sometimes referred to as an API key, controls the consumers sending requests to the Gateway.\n  Feel free to change the policies used and experiment further implementing policies like caching OIDC-based authentication, canary, GraphQL integration, and more with the extensive list of plugins provided by Kong. Check the list over here: https://docs.konghq.com/hub/\nThen we\u0026rsquo;re going to apply other policies to our previously created Ingress\n Set up Ingress rule for Kong Ingress\n Configuring a fallback service\n Configuring HTTPS redirect for services\n Using Redis for rate-limiting\n  "
},
{
	"uri": "/090_cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Workshop Cleanup Congratulations on completing the workshop! The next few sections will instruct you how to turn off all the infrastructure you\u0026rsquo;ve created in order to work through the material.\nUninstall ELK  helm uninstall elk -n elk helm uninstall logstash -n elk helm uninstall kibana -n elk kubectl delete namespace elk  Uninstall Prometheus Operator  kubectl delete kongclusterplugin prometheus kubectl delete service kong-dp-monitoring -n kong-dp kubectl delete servicemonitor kong-dp-service-monitor -n kong-dp kubectl delete serviceaccount kong-prometheus -n kong-dp kubectl delete clusterrole prometheus kubectl delete clusterrolebinding prometheus kubectl delete prometheus kong-dp-prometheus -n kong-dp helm uninstall prometheus -n prometheus kubectl delete namespace prometheus  Uninstall Redis  kubectl delete deployment redis -n redis kubectl delete service redis -n redis kubectl delete namespace redis  Uninstall Kong for Kubernetes  kubectl delete ingress route1 kubectl delete service route1-ext kubectl delete service sample kubectl delete deployment sample kubectl annotate ingress sampleroute konghq.com/plugins- kubectl delete kongplugin rl-by-minute kubectl delete kongplugin apikey kubectl delete secret consumerapikey kubectl delete kongconsumer consumer1 kubectl delete kongclusterplugin tcp-log helm uninstall kong -n kong helm uninstall kong-dp -n kong-dp kubectl delete -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl delete namespaces kong kong-dp  Delete the EKS Cluster  eksctl delete cluster --name K4K8S  "
},
{
	"uri": "/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]